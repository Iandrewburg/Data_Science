{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729093cb-d82d-4aaa-a7a0-e93b28230e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949f6b3-2330-4a19-872d-a958d30605ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated features\n",
    "def generateRandomCovarianceVarianceMatrix(n_features):\n",
    "    random_matrix = np.random.rand(n_features, n_features)\n",
    "\n",
    "    # Create a symmetric matrix by taking the average of the matrix and its transpose\n",
    "    var_cov_matrix = (random_matrix + random_matrix.T) / 2\n",
    "    \n",
    "    # Ensure the diagonal elements represent variances (non-negative values)\n",
    "    var_cov_matrix[np.diag_indices(n_features)] = np.abs(var_cov_matrix[np.diag_indices(n_features)])\n",
    "\n",
    "    # Ensure positive semidefiniteness\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(var_cov_matrix)\n",
    "    eigenvalues[eigenvalues < 0] = 0  # Replace negative eigenvalues with 0\n",
    "    var_cov_matrix = np.dot(eigenvectors, np.dot(np.diag(eigenvalues), eigenvectors.T))\n",
    "\n",
    "    return var_cov_matrix\n",
    "\n",
    "def generateCorrelatedFeatures(n_features, n_samples, vc_matrix=None):\n",
    "    if vc_matrix is None:\n",
    "        vc_matrix = generateRandomCovarianceVarianceMatrix(n_features)\n",
    "        \n",
    "    return np.random.multivariate_normal(np.repeat(0, n_features), vc_matrix, size=n_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652919d8-9e3f-410c-8fca-d6fae54cdaca",
   "metadata": {},
   "source": [
    "## Illustration of PCA on a simple simulated dataset with 2 correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee7a7d-43b3-461c-888f-17af2e56a752",
   "metadata": {},
   "source": [
    "### Data generation (2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c93b10-1d08-45e6-bd66-752a06200572",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20240304)\n",
    "\n",
    "# Generate data\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "X = generateCorrelatedFeatures(n_features, n_samples)\n",
    "\n",
    "def f(X):\n",
    "    return X[:, 0] + X[:, 1]\n",
    "\n",
    "Y = f(X) + np.random.normal(size=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540abbb4-3757-4004-95e5-e5617a48e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "ax1.scatter(X[:, 0], Y, alpha=0.5)\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Target')\n",
    "\n",
    "ax2.scatter(X[:, 1], Y, alpha=0.5)\n",
    "ax2.set_xlabel('Feature 2')\n",
    "\n",
    "fig.suptitle('Data after PCA')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0c554-1d2a-457b-bb32-83c24718c689",
   "metadata": {},
   "source": [
    "### Running PCA (2 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a503d-b3ef-4fb2-8ac8-74eeba3e7487",
   "metadata": {},
   "source": [
    "We transform the coordinates of the original variables to capture as much\n",
    "variation as we can with independent (orthogonal) dimensions.\n",
    "For a very nice illustration and discussion, see [here](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38b29a-6062-4bfb-9f50-bef18a3597a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and calculate the principal components as X_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433b88e-5dad-4534-9971-864349552a95",
   "metadata": {},
   "source": [
    "### Characteristics of PCA\n",
    "\n",
    "Recap: We transform the coordinates of the original variables to capture as much variation as we can with independent (orthogonal) dimensions.\n",
    "For a very nice illustration and discussion, see [this Cross Validated post](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df110f-3e51-432c-a2a7-6dd82e5b3897",
   "metadata": {},
   "source": [
    "#### Plot principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b5ef0-04be-4224-8836-f0458872892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA-transformed data\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "    \n",
    "fig.suptitle('Data after PCA')\n",
    "\n",
    "ax1.scatter(X_pca[:, 0], Y, color='red', alpha=0.5)\n",
    "ax1.set_xlabel('Principal Component 1')\n",
    "ax1.set_ylabel('Target')\n",
    "\n",
    "ax2.scatter(X_pca[:, 1], Y, color='red', alpha=0.5)\n",
    "ax2.set_xlabel('Principal Component 2')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6837934-7718-44c3-afb3-570e78b59a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the space of X to the space of principal components\n",
    "fig, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=True)\n",
    "    \n",
    "fig.suptitle('Transform X into PC')\n",
    "\n",
    "ax1.scatter(X[:, 0], X[:, 1], color='blue', alpha=0.5)\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "\n",
    "# add the principal component transformation vectors to the plot once you know where to get the loadings from\n",
    "\n",
    "# add a new plot about the transformed features\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d998b1e-ed07-42c7-8cc7-eeebe9207c45",
   "metadata": {},
   "source": [
    "#### Variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f1228-0a5c-4d9e-957f-4bc7658abae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance_in_features = np.var(X[:, 0]) + np.var(X[:, 1])\n",
    "total_variance_in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c76408-ec77-4521-9784-aa48cf2d07e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373651f-35e0-43cd-b941-5f630da24454",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ / total_variance_in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000b9e8-296e-4d56-99b9-ca146c5c8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03b857-a28d-420d-b7fd-06120df68314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The first principal component explains {round(pca.explained_variance_ratio_[0] * 100, 1)}% of the total variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84153ee-f242-43db-aba0-bf9b4fac4258",
   "metadata": {},
   "source": [
    "#### Loadings/weights: how to transform the space of X into the space of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7514981-6347-4cbc-a0d3-a772f9b93675",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f0c63-fe32-4f6f-8a01-499013b6d30b",
   "metadata": {},
   "source": [
    "The squared loadings (weights) sum up to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6274a-c60e-4a58-a862-b40a7d0628cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.components_**2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd63da-519a-4d35-bc2d-0f82068ae3d4",
   "metadata": {},
   "source": [
    "#### Orthogonality of principal components\n",
    "\n",
    "Each principal component contains \"independent\" variance from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2c777-7ed0-4b26-bbad-d5872dc9c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca.components_[:, 0] * pca.components_[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74f3a3-72e0-4465-bc41-c2faffb817c8",
   "metadata": {},
   "source": [
    "## PCA as regularization (complexity reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd864ef-65c2-416e-a900-2e49dd1225d0",
   "metadata": {},
   "source": [
    "### Multivariate model\n",
    "\n",
    "We work with an \"approximately sparse\" model: all features matter but to an exponentially decreasing extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95da0f6-9ecc-47c5-a760-8d81bfa72bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCoefficients(n_features):\n",
    "    return [4 / (i + 1)**2 for i in range(n_features)]\n",
    "\n",
    "def approximatelySparseF(X):\n",
    "    beta = generateCoefficients(X.shape[1])\n",
    "    return np.dot(X, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ab7aa3-1654-4d25-8876-b2804aa09d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([i + 1 for i in range(10)], generateCoefficients(10))\n",
    "plt.ylabel(\"Beta coefficient\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f0e78d-2790-4b4c-b726-52bc9e5658cd",
   "metadata": {},
   "source": [
    "### Uncorrelated predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735950ed-3331-48c0-a097-71bcd65d8e81",
   "metadata": {},
   "source": [
    "#### Data generation (uncorrelated predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a7e14-73a7-4de6-8700-f467208a3f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20240304)\n",
    "\n",
    "n_samples = 100\n",
    "n_features = 40\n",
    "\n",
    "X_uncorrelated = np.random.normal(0, 1, size=(n_samples, n_features)) # uncorrelated features\n",
    "Y = approximatelySparseF(X_uncorrelated) + np.random.normal(size=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e5d71-dc84-40df-a358-eb722dffa563",
   "metadata": {},
   "source": [
    "#### Running PCA (uncorrelated predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f440de-2052-43a1-89cb-b578831f5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_uncorrelated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb5547-b377-4aa7-9240-a962e492b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c9c74-abdc-420b-8738-7028494bc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca95 = PCA(n_components=0.95)\n",
    "X_pca95 = pca95.fit_transform(X_uncorrelated)\n",
    "X_pca95.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d5026-2173-4ac7-957b-af0b12576c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method without the need of creating a new PCA object\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "np.argmax(cumulative_explained_variance > 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30526701-7b9e-4b7c-8a04-a3709458fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{#TODO / n_features * 100, 1)}% of features explain 95% of the total variance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c53fb6-3892-4261-9536-492752bdfc72",
   "metadata": {},
   "source": [
    "### Correlated predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01277e1b-1c62-4127-ba19-6e3b90dab6d6",
   "metadata": {},
   "source": [
    "#### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c0b2f-5014-4e64-8e25-ecbe4c63dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20240304)\n",
    "\n",
    "vc_matrix_fixed = generateRandomCovarianceVarianceMatrix(n_features)\n",
    "X_correlated = generateCorrelatedFeatures(n_features, n_samples, vc_matrix_fixed)\n",
    "Y = approximatelySparseF(X_correlated) + np.random.normal(size=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8966959b-8db5-466c-bf84-f37ddc77b9bd",
   "metadata": {},
   "source": [
    "#### Running PCA (correlated predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32beb77c-43f5-4332-b91b-8ca3b46e0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run PCA and look at what percentage of the features explain 95%+ of the total variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfc32e-65a5-4990-8688-4f76e5a1fb99",
   "metadata": {},
   "source": [
    "### Estimate Y with all variables vs with the first few principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c2149d-3c4b-43ee-b374-7bd03e553088",
   "metadata": {},
   "source": [
    "#### How to evaluate?\n",
    "\n",
    "We used to evaluate our models in two different ways:\n",
    "\n",
    "1. Monte Carlo simulation: generating many different data sets from the same data generation process and evaluating the performance at specific points, using our knowledge of the true outcome (with the irreducible error removed) - we could look at bias, variance and MSE.\n",
    "2. Train-test split: take a data set and split it into two parts: use the train set to estimate the model and evaluate the performance on the test set (the result includes the irreducible error) - we could look at MSPE only.\n",
    "\n",
    "Here we take the combination of these two approaches: We generate a 'test set' from the data generation process, and then evaluate the model on this set, using our knowledge of the true outcome (with the irreducible error removed). This allows us to focus on the irreducible part of the error, while still sticking to our one data set. We will look at the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10697250-ed20-4655-a1b9-a3521e24d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test set\n",
    "X_test = generateCorrelatedFeatures(n_features, n_samples, vc_matrix_fixed)\n",
    "Y_test = approximatelySparseF(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87077c35-0bbf-4815-bcab-0666db9f7ce8",
   "metadata": {},
   "source": [
    "#### OLS on all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2730b-4064-444a-aac8-8f45783a6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg_all_features = LinearRegression().fit(X_correlated, Y)\n",
    "prediction = linreg_all_features.predict(X_test)\n",
    "mse_all = np.mean((prediction - Y_test)**2)\n",
    "mse_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d8e61-26f5-44c6-98d1-42e39dcd51d4",
   "metadata": {},
   "source": [
    "#### OLS on the first few principal components that capture 95% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c96fe-3550-457f-8656-ddd33a551508",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_pca95 = LinearRegression().fit(X_pca[:,:n_pca_95], Y)\n",
    "prediction = # TODO\n",
    "mse_pca95 = np.mean((prediction - Y_test)**2)\n",
    "mse_pca95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3985b192-2f4e-4cd0-9768-2d9647a53f0e",
   "metadata": {},
   "source": [
    "#### Bias-variance trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddec8599-fec9-4416-bd3c-877039fdcddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate MSE of OLS models by changing how many principal components is used\n",
    "mse_results = []\n",
    "for i in range(n_features):\n",
    "    n_pc = i + 1\n",
    "    # estimate linear regression\n",
    "    # predict on X_test (do not forget to use the PC-s on X_test instead of the original features)\n",
    "    # calculate MSE as mse_pca\n",
    "    mse_results.append(mse_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40a192-4bea-414a-903b-40fccf3adcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart\n",
    "plt.plot([i + 1 for i in range(n_features)], mse_results)\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Bias-variance trade-off in Principal Component Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f0008-a9be-4ad7-a9ec-45efd3866050",
   "metadata": {},
   "source": [
    "#### How would LASSO perform on this task?\n",
    "\n",
    "Try it out at home ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be0533-70cf-4a17-9d0e-990f990c6ae1",
   "metadata": {},
   "source": [
    "## PCA on real data\n",
    "\n",
    "From the ISLR website, we can download a gene expression data set (Ch10Ex11.csv) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.\n",
    "\n",
    "We would have no chance to estimate any model on these 1,000 features. However, we could reduce the dimensionality with PCA. Then, we could look at the relation of the first few principal components (that captures part of the variance of _all_ 1,000 features) and the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94924ee-838d-441a-8958-f60c72247943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "url = 'https://www.statlearning.com/s/Ch10Ex11.csv'\n",
    "genes = pd.read_csv(url, header=None)\n",
    "\n",
    "# Transpose the dataframe and convert to pandas DataFrame\n",
    "genes = genes.T\n",
    "genes = pd.DataFrame(genes)\n",
    "print('Dimensions of genes dataframe:', genes.shape)\n",
    "\n",
    "# Define health_status\n",
    "health_status = ['healthy'] * 20 + ['diseased'] * 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6b635-ca86-40e8-ba6b-e1c83a480fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply PCA on the genes data and create pca_genes containing the first two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faabd096-3eeb-4a83-9e31-1def9a23d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_genes[:, 0], pca_genes[:, 1], c=['red' if val == 'diseased' else 'green' for val in health_status], alpha=0.5)\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Health status by the first two principal components')\n",
    "\n",
    "# Add empty scatter for legend\n",
    "plt.scatter([], [], color='red', label='diseased')\n",
    "plt.scatter([], [], color='green', label='healthy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
