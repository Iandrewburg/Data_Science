{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3f7316",
   "metadata": {},
   "source": [
    "## Predict the demand for bike share using known tools (cont.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81edd3b6",
   "metadata": {},
   "source": [
    "### Recreating data from last class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62650352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sample data\n",
    "bike_data = pd.read_csv(\"https://raw.githubusercontent.com/divenyijanos/ceu-ml/2023/data/bike_sharing_demand/bike_sample.csv\")\n",
    "features = bike_data.drop(columns=[\"count\", \"registered\", \"casual\"]).select_dtypes(include=np.number)\n",
    "label = bike_data[\"count\"]\n",
    "\n",
    "prng = np.random.RandomState(20240306)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=prng)\n",
    "\n",
    "# feature engineered data\n",
    "def extract_dt_features(df_with_datetime):\n",
    "    df_with_datetime['datetime'] = pd.to_datetime(df_with_datetime['datetime'], utc=True)\n",
    "    df_with_datetime['year'] = df_with_datetime['datetime'].dt.year\n",
    "    df_with_datetime['day'] = df_with_datetime['datetime'].dt.day\n",
    "    df_with_datetime['month'] = df_with_datetime['datetime'].dt.month\n",
    "    df_with_datetime['hour'] = df_with_datetime['datetime'].dt.hour\n",
    "    df_with_datetime['dayofweek'] = df_with_datetime['datetime'].dt.dayofweek\n",
    "\n",
    "\n",
    "extract_dt_features(bike_data)\n",
    "\n",
    "feature_matrix = bike_data.drop(columns=[\"count\", \"registered\", \"casual\"]).select_dtypes(include=np.number)\n",
    "label = bike_data[\"count\"]\n",
    "prng = np.random.RandomState(20240306)\n",
    "X_train_fe, X_test_fe, y_train, y_test = train_test_split(feature_matrix, label, test_size=0.2, random_state=prng)\n",
    "\n",
    "# full data\n",
    "bike_full = pd.read_csv(\"https://raw.githubusercontent.com/divenyijanos/ceu-ml/2023/data/bike_sharing_demand/train.csv\")\n",
    "extract_dt_features(bike_full)\n",
    "\n",
    "full_data_without_original_test = bike_full.loc[~bike_full.datetime.isin(bike_data.filter(X_test.index, axis=0)['datetime'])]\n",
    "full_data_without_original_test.shape\n",
    "\n",
    "X_full = full_data_without_original_test.drop(columns=[\"count\", \"registered\", \"casual\", \"datetime\"])\n",
    "y_full = full_data_without_original_test['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2446f-f300-4d6f-9145-f934c1dc8640",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86b2bd-25c9-49f2-a57a-657281545bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def calculateRMSLE(prediction, y_obs):\n",
    "    return round(np.sqrt(\n",
    "        np.mean(\n",
    "            (\n",
    "                np.log(np.where(prediction < 0, 0, prediction) + 1) - \n",
    "                np.log(y_obs + 1)\n",
    "            )**2\n",
    "        )\n",
    "    ), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd530a",
   "metadata": {},
   "source": [
    "### Model #5: Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import tree\n",
    "\n",
    "steps = [\n",
    "    (\"tree\", tree.DecisionTreeRegressor(max_depth=5, random_state=prng))\n",
    "]\n",
    "pipe_tree = Pipeline(steps)\n",
    "\n",
    "models = [\"Tree\", \"Feature engineered tree\", \"Feature engineered tree large n\"]\n",
    "datasets = [\n",
    "    (X_train, y_train, X_test),\n",
    "    (X_train_fe, y_train, X_test_fe),\n",
    "    (X_full, y_full, X_test_fe)\n",
    "]\n",
    "\n",
    "tree_results = []\n",
    "\n",
    "for model, data in zip(models, datasets):\n",
    "\n",
    "    # TODO: fit the tree model\n",
    "\n",
    "    train_error = calculateRMSLE(pipe_tree.predict(data[0]), data[1])\n",
    "    test_error = calculateRMSLE(pipe_tree.predict(data[2]), y_test)\n",
    "\n",
    "    tree_result = [model, train_error, test_error]\n",
    "    tree_results.append(tree_result)\n",
    "\n",
    "pd.DataFrame(tree_results, columns = [\"Model\", \"Train\", \"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e4f124-f37a-40db-b283-c1b5783320a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(pipe_tree[\"tree\"], feature_names = X_full.columns.to_list(), max_depth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc628dd-c76a-4e28-b0c1-d59880ed4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fit a more appropriate model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18e750-2c3f-48d8-b425-7eba077a9034",
   "metadata": {},
   "source": [
    "### Model #6: Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e0d52-6c7d-4dc0-b9ae-d5d18accd74b",
   "metadata": {},
   "source": [
    "#### Default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5cf395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "steps = [\n",
    "    (\"random_forest\", RandomForestRegressor())\n",
    "]\n",
    "pipe_rf = Pipeline(steps)\n",
    "\n",
    "models = [\"RF\", \"Feature engineered RF\", \"Feature engineered RF large n\"]\n",
    "\n",
    "rf_results = []\n",
    "\n",
    "for model, data in zip(models, datasets):\n",
    "\n",
    "    pipe_rf.fit(data[0], data[1])\n",
    "\n",
    "    train_error = calculateRMSLE(pipe_rf.predict(data[0]), data[1])\n",
    "    test_error = calculateRMSLE(pipe_rf.predict(data[2]), y_test)\n",
    "\n",
    "    rf_result = [model, train_error, test_error]\n",
    "    rf_results.append(rf_result)\n",
    "\n",
    "pd.DataFrame(rf_results, columns = [\"Model\", \"Train\", \"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad674c7-4af6-4708-80ae-ce91effc0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at single trees\n",
    "chosen_tree = pipe_rf[\"random_forest\"].estimators_[1]\n",
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(chosen_tree, feature_names = X_full.columns.to_list(), max_depth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44857e9e-8d11-4c51-88fc-0a113e861709",
   "metadata": {},
   "outputs": [],
   "source": [
    "(chosen_tree.tree_.max_depth, chosen_tree.tree_.node_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc6e42-ac49-4cdf-ad0a-25211442959f",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc93935-931a-4b3b-9df3-ecafe4ea98ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [0.05, 0.2, 0.5, 0.7]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 100, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f5380-871a-4445-a109-a27c766b7237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter tuning - fit the models on the full sample\n",
    "rf = RandomForestRegressor(random_state=prng)\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation,\n",
    "# search across 50 different combinations, and use all available cores, evaluate by RMSLE\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=50, cv=5, scoring=\"neg_root_mean_squared_log_error\", verbose=2, random_state=prng, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be21f2-ae16-4d34-a831-40465010dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4b7d3-ca7b-4d82-9f96-ce4d5bfc5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rf_random.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1573ea-73d1-4c77-9e16-0914857f1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = calculateRMSLE(rf_random.best_estimator_.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(rf_random.best_estimator_.predict(X_test_fe), y_test)\n",
    "cv_rf_result = [\"CV RF large n\", train_error, test_error]\n",
    "\n",
    "rf_results.append(cv_rf_result)\n",
    "pd.DataFrame(rf_results, columns = [\"Model\", \"Train\", \"Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930b59e-c7b9-4de3-b7f0-df09047c3380",
   "metadata": {},
   "source": [
    "### Model #7: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0e210-df3a-4e98-bb38-75afde267c1d",
   "metadata": {},
   "source": [
    "#### Technical detour: category type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1d905-dc90-48fa-a8db-0f4bfa39bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_features = [\"season\", \"holiday\", \"workingday\", \"weather\", \"year\", \"month\", \"day\", \"hour\", \"dayofweek\"]\n",
    "X_full[dummy_features] = X_full[dummy_features].astype(\"category\")\n",
    "X_full.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832cda3-ba62-43ce-89ff-1c49d0b9ef2d",
   "metadata": {},
   "source": [
    "Why we did not deal with this so far? Because sklearn's implementation of trees [does not handle them differently](https://scikit-learn.org/stable/modules/tree.html#:~:text=and%20categorical%20data.-,However%2C%20the%20scikit%2Dlearn%20implementation%20does%20not%20support%20categorical%20variables%20for%20now.,-Other%20techniques%20are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0653e-8dc6-4932-8819-1aa852cb5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration\n",
    "pipe_tree_deep.fit(X_full, y_full)\n",
    "\n",
    "train_error = calculateRMSLE(pipe_tree_deep.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(pipe_tree_deep.predict(X_test_fe), y_test)\n",
    "\n",
    "tree_results.append([\"Deep tree large n categories\", train_error, test_error])\n",
    "\n",
    "pd.DataFrame(tree_results, columns = [\"Model\", \"Train\", \"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22ee62-751f-4e35-a26b-4f882cf7ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "tree.plot_tree(pipe_tree_deep[\"deep_tree\"], feature_names = X_full.columns.to_list(), max_depth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2276ff-ed19-479c-9820-627faff6e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tree: xgboost\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBRegressor(enable_categorical=True).fit(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26355fb2-34f8-43e7-a5cb-1740b0eac6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = calculateRMSLE(xgb_model.predict(X_full), y_full)\n",
    "test_error = calculateRMSLE(xgb_model.predict(X_test_fe), y_test)\n",
    "\n",
    "[\"XGB\", train_error, test_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f15a39-7931-40bc-9b59-d1856cbda74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.get_booster().trees_to_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8cf2e-b564-46b9-a782-385465fd725d",
   "metadata": {},
   "source": [
    "### Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ba592-3a51-4210-9405-3daebc8ccb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "to_submit = pd.DataFrame({\n",
    "    'datetime': bike_test.datetime.dt.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'count': \n",
    "})\n",
    "to_submit.to_csv('', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71610c04",
   "metadata": {},
   "source": [
    "## Predict heart failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8906fef",
   "metadata": {},
   "source": [
    "We are going to work with a heart disease data set collected from 5 different sources (for more detailed information consult [Kaggle](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)). It contains health condition data about 460 patients and whether they got a heart attack. We will use the subset of this dataset and preserve the rest for evaluating our final model on Kaggle.\n",
    "\n",
    "Cardiovascular diseases (CVDs) are [the most common cause of death globally](https://ourworldindata.org/cardiovascular-diseases) making this task particularly relevant. As the competition states: _\"People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors) need early detection and management wherein a machine learning model can be of great help.\"_\n",
    "\n",
    "Attribute Information:\n",
    "- `Age`: age of the patient [years]\n",
    "- `Sex`: sex of the patient [M: Male, F: Female]\n",
    "- `ChestPainType`: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
    "- `RestingBP`: resting blood pressure [mm Hg]\n",
    "- `Cholesterol`: serum cholesterol [mm/dl]\n",
    "- `FastingBS`: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
    "- `RestingECG`: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
    "- `MaxHR`: maximum heart rate achieved [Numeric value between 60 and 202]\n",
    "- `ExerciseAngina`: exercise-induced angina [Y: Yes, N: No]\n",
    "- `Oldpeak`: oldpeak = ST [Numeric value measured in depression]\n",
    "- `ST_Slope`: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
    "- `HeartDisease`: output class [1: heart disease, 0: Normal]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c21df6-f9e3-49ec-847c-77e9e85a18bd",
   "metadata": {},
   "source": [
    "### Know your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928623b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "heart_data = pd.read_csv(\"../data/heart_failure/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d81761-6e3e-402b-af4a-3160a33a63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8f261-7013-4cf0-b374-fd5d4ff995f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69560847-f1fd-4469-882f-078dd7830c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(heart_data, alpha=0.2, figsize=(12, 12), diagonal=\"kde\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17263c47-57e8-4c26-9951-cc8664943c92",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae7d9c-a044-4570-972c-03a0cbd1046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = heart_data.drop(columns=[\"HeartDisease\"])\n",
    "label = heart_data[\"HeartDisease\"]\n",
    "\n",
    "prng = np.random.RandomState(20240311)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=prng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53605cd4-bb28-4b44-9d8c-ed4df0710040",
   "metadata": {},
   "source": [
    "### Benchmark evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculateAccuracy(observed, predicted):\n",
    "    return round(accuracy_score(observed, predicted), 4)\n",
    "\n",
    "benchmark_prediction = np.bincount(y_train).argmax()\n",
    "train_accuracy = calculateAccuracy(y_train, np.repeat(benchmark_prediction, len(y_train)))\n",
    "test_accuracy = calculateAccuracy(y_test, np.repeat(benchmark_prediction, len(y_test)))\n",
    "[train_accuracy, test_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe71e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, np.repeat(benchmark_prediction, len(y_test)))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6f87c-d3a6-458a-ad4d-d1fa39c195e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa42db-8897-4eec-93d2-bcf573b9ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def calculateMetrics(observed, predicted):\n",
    "    return {\n",
    "        \"accuracy\": calculateAccuracy(observed, predicted),\n",
    "        \"precision\": round(precision_score(observed, predicted), 4),\n",
    "        \"recall\": round(recall_score(observed, predicted), 4)\n",
    "    }\n",
    "\n",
    "pd.DataFrame(\n",
    "    [calculateMetrics(y, np.repeat(benchmark_prediction, len(y))) for y in [y_train, y_test]],\n",
    "    index=[\"Train\", \"Test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed696d-82c7-4926-98f3-59cb83e05e77",
   "metadata": {},
   "source": [
    "### Benchmark #2: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948d6ba-98a2-459a-9889-d1ee69994b6f",
   "metadata": {},
   "source": [
    "#### Pipeline with preprocessing and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d315e0-f72a-4587-80be-2570eef0a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
    "categorical_vars = heart_data.select_dtypes(include=\"object\").columns.to_list()\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [(\"create_dummies\", one_hot_encoder, categorical_vars)],\n",
    "    remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d60ff-e116-481a-abab-f0166dd835d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe_logit = Pipeline([\n",
    "    # TODO: create dummies, scale, then estimate without any penalty\n",
    "])\n",
    "pipe_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae254f-e797-4221-a58a-a81c3b7a03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_logit.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed3579-c100-4a50-a4ce-3bad569b0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    [calculateMetrics(y, pipe_logit.predict(x)) for x, y in [(X_train, y_train), (X_test, y_test)]],\n",
    "    index=[\"Train\", \"Test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306a690-30e3-478a-b3ed-719bcf4b3100",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414da4b-c3a7-4dfe-9992-d9391803d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use a different cutoff we need to predict probabilities first\n",
    "pipe_logit.predict_proba(X_train)[:5, :]  # look at first 5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87288eff-67d0-437c-b36b-5b7bb5c34d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictProbs(model, X):\n",
    "    return model.predict_proba(X)[:, 1]\n",
    "\n",
    "def predictWithCutoff(model, X, cutoff):\n",
    "    return (predictProbs(model, X) >= cutoff).astype(int)\n",
    "\n",
    "cutoff = 0.3\n",
    "pd.DataFrame(\n",
    "    [calculateMetrics(y, predictWithCutoff(pipe_logit, x, cutoff)) for x, y in [(X_train, y_train), (X_test, y_test)]],\n",
    "    index=[\"Train\", \"Test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1954aa-8b05-4aa7-bb69-f7f98cc15f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_scores = predictProbs(pipe_logit, X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores) \n",
    "print(f\"thresholds: {thresholds}\\n\")\n",
    "print(f\"FPR: {fpr}\\n\")\n",
    "print(f\"TPR: {tpr}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33a4c5-0282-4280-b04d-b1d6413cf658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "roc_display = RocCurveDisplay.from_estimator(pipe_logit, X_test, y_test)\n",
    "plt.show(roc_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0b4fd-6b2b-47a6-a77d-bb9111097566",
   "metadata": {},
   "source": [
    "#### Precision-recall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c8f53-a577-4aeb-96df-190486b6bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve\n",
    "\n",
    "PrecisionRecallDisplay.from_estimator(pipe_logit, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424ff56-ce7f-4389-9e9c-7a0b353001b9",
   "metadata": {},
   "source": [
    "#### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175e46b-4f88-438a-8bcd-533a47bd1e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "change_zero_cholesterol_to_closest_neighbors = KNNImputer(missing_values=0)\n",
    "\n",
    "column_transformer_with_imputation = ColumnTransformer(\n",
    "    [\n",
    "        (\"create_dummies\", one_hot_encoder, categorical_vars),\n",
    "        (\"impute_cholesterol\", change_zero_cholesterol_to_closest_neighbors, [\"Cholesterol\"])\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "pipe_logit_with_imputation = Pipeline([\n",
    "    (\"preprocess\", column_transformer_with_imputation),\n",
    "    (\"scale\", MinMaxScaler()),\n",
    "    (\"logit\" , LogisticRegression(penalty=None, random_state=prng))\n",
    "])\n",
    "pipe_logit_with_imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e5e3d-879a-4824-84bd-2eca8c91e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_logit_with_imputation.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c1061-9975-4ea1-9b4c-b95119039cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_display_imputed = RocCurveDisplay.from_estimator(pipe_logit_with_imputation, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a03cf-311f-4bba-b7eb-ba10f08d2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "roc_display.plot(ax=ax, name=\"Logit\")\n",
    "roc_display_imputed.plot(ax=ax, name=\"Logit with imputation\")\n",
    "plt.title('ROC Curves for Two Estimators')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2853a8-1a68-40d1-a683-893efbe4780c",
   "metadata": {},
   "source": [
    "### autoML\n",
    "Some of the ML processes are easy to automate (e.g. hyper-parameter tuning, model evaluation and selection). There are many tools that solve this automation task, e.g. the [auto-sklearn package](https://automl.github.io/auto-sklearn/master/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
