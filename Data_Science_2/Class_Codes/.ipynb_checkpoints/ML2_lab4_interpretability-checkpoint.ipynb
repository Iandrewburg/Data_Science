{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1695f3b1-2e1f-4448-8a56-1d70f423cb7e",
   "metadata": {},
   "source": [
    "## Interpretability: How to make more money?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb759a-bfac-4d3d-ac72-91ddeb1b6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# fetch dataset (takes a while)\n",
    "adult = fetch_openml(name=\"adult\", version=2) \n",
    "\n",
    "adult.frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8f53a-124c-4b2f-b30c-b18a74896e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adult.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624b9e5-959b-49ba-b0ef-582cc6613ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = adult.frame[\"class\"]\n",
    "X = adult.frame.drop(columns=[\"class\"])\n",
    "\n",
    "prng = np.random.RandomState(20240325)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=prng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb890691-12f9-4c6a-b86c-81324823d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56e817-dc8e-4d55-9c16-6b0638a90917",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbad65-471f-4c98-8c20-7ae5abddd491",
   "metadata": {},
   "source": [
    "### Benchmark: logistic regression (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de78546-add6-4001-ac5a-36e937f1ddb6",
   "metadata": {},
   "source": [
    "By default, sklearn Transformers return Numpy arrays. Thus, even though we input a Pandas DataFrame with the names of the features, we will lose them on the pipeline execution. This is a bad thing when we need to explain our model using, for example, feature importances, because we are no able to track feature names natively.\n",
    "You can explicitly set the output to keep the Pandas output with `set_config`. [Source](https://medium.com/@anderson.riciamorim/how-to-keep-feature-names-in-sklearn-pipeline-e00295359e31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2556181d-a2b7-4609-b175-455c13148fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output=\"pandas\")  # ensure that the transformations return pandas df instead of numpy arrays (thus, they preserve column names e.g.)\n",
    "\n",
    "# preprocess\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, drop=\"first\", handle_unknown=\"ignore\")\n",
    "categorical_vars = X.select_dtypes(include=\"category\").columns.to_list()\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [(\"create_dummies\", one_hot_encoder, categorical_vars)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "pipe_logit = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"preprocess\", column_transformer),\n",
    "    (\"scale\", MinMaxScaler()),\n",
    "    (\"logit\", LogisticRegression(penalty=None, random_state=prng))\n",
    "])\n",
    "pipe_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb8247-5511-4887-9978-3f364b3a64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef07c4d-c4a0-4817-9d18-1e1bc78f59f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "\n",
    "predicted_probs = pipe_logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score_test = roc_auc_score(y_test, predicted_probs)\n",
    "print(f\"AUC on the test set for simple logit is {round(auc_score_test, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6291c79-f11f-48a5-ba10-fa3a99deebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_logit[\"logit\"].coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0a8c7-e4ff-46c4-8f9c-c0a4bebeb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_logit[\"logit\"].feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2922607-2992-40b5-be05-4e2f52b726f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only categories that have enough observations, put the others into an \"infrequent\" group\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, drop=\"first\", min_frequency=100, handle_unknown=\"infrequent_if_exist\")\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [(\"create_dummies\", one_hot_encoder, categorical_vars)],\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False  # keep variable names shorter\n",
    ")\n",
    "\n",
    "pipe_logit = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"preprocess\", column_transformer),\n",
    "    (\"scale\", MinMaxScaler()),\n",
    "    (\"logit\", LogisticRegression(penalty=None, random_state=prng, max_iter=1000))\n",
    "])\n",
    "pipe_logit.fit(X_train, y_train)\n",
    "pipe_logit[\"logit\"].feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f34f1b-3a3e-46aa-ac07-f2df673740de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs = pipe_logit.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score_test = roc_auc_score(y_test, predicted_probs)\n",
    "print(f\"AUC on the test set for the interpretable logit is {round(auc_score_test, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee8926-06d7-44bf-b855-66b353912606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_summary = pd.DataFrame({\n",
    "    \"feature\": pipe_logit[\"logit\"].feature_names_in_,\n",
    "    \"coefficient\": pipe_logit[\"logit\"].coef_[0]\n",
    "})\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe0857-5cdb-45f6-b839-e4525cb407e7",
   "metadata": {},
   "source": [
    "The logistic regression model uses the sigmoid function to transform the real line into [0, 1]\n",
    "\n",
    "$$\n",
    "P(Y=1) = sigmoid(\\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k)\n",
    "$$\n",
    "\n",
    "As \n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "with some restructuring we can get a result showing that logistic regression is a linear model for the log odds:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\log(odds) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\n",
    "$$\n",
    "\n",
    "From here we can derive that a change in a feature by one unit changes the odds ratio (multiplicative) by a factor of the exponential of its coefficient:\n",
    "\n",
    "$$\n",
    "\\frac{odds(x_j + 1)}{odds(x_j)} = \\exp(\\beta_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34613310-f0b2-4e65-ba5a-84fab250aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary[\"odds_ratio\"] = np.exp(model_summary[\"coefficient\"])\n",
    "model_summary.sort_values(by=\"odds_ratio\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033230ef-8a33-4c17-8759-6455f9bf73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "top_coeffs = model_summary.sort_values(by=\"coefficient\", ascending=True).tail(20)\n",
    "plt.barh(top_coeffs.feature, top_coeffs.odds_ratio)\n",
    "plt.xlabel(\"Odds ratio\")\n",
    "plt.title(\"Odds ratio of the top 20 positive factors\")\n",
    "plt.axvline(x=1, color=\"darkred\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd87ce-251b-4179-8881-afa8b4c9dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_coeffs = model_summary.sort_values(by=\"coefficient\", ascending=True).head(20)\n",
    "plt.barh(bottom_coeffs.feature, bottom_coeffs.odds_ratio)\n",
    "plt.xlabel(\"Odds ratio\")\n",
    "plt.title(\"Odds ratio of the top 20 negative factors\")\n",
    "plt.axvline(x=1, color=\"darkred\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cd6bf-91bf-44bd-a97d-aa185b61efd5",
   "metadata": {},
   "source": [
    "Is scaling a good idea?\n",
    "Without scaling the coefficients could be easier to interpret. However, convergence is much harder to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c710fa0-9d77-4a43-a1c2-2183c4eec64b",
   "metadata": {},
   "source": [
    "### Permutation-based feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea608b4-a3be-40c5-9065-3d2201028dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Found unknown categories*\")\n",
    "    varimp_permutation = permutation_importance(pipe_logit, X_test, y_test, n_repeats=10, scoring=\"roc_auc\", random_state=prng)   # could run in parallel with n_jobs=-1 but then suppressing the warnings is more complicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d15f5d-7552-46bc-adbd-9faa5d914151",
   "metadata": {},
   "outputs": [],
   "source": [
    "varimp_sorted_idx = varimp_permutation.importances_mean.argsort()\n",
    "\n",
    "plt.boxplot(\n",
    "    varimp_permutation.importances[varimp_sorted_idx].T, \n",
    "    labels=X.columns[varimp_sorted_idx],\n",
    "    vert=False\n",
    ")\n",
    "plt.axvline(x=0, color=\"darkred\", linestyle=\"--\")\n",
    "plt.xlabel(\"Decrease in AUC (ROC) score\")\n",
    "plt.title(\"Permutation based feature importance (test set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b426a-81a1-46b8-aaf4-a78723fd2802",
   "metadata": {},
   "source": [
    "### Partial dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882f50f-a7b9-4e99-9bfb-3dbb508a5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Found unknown categories*\")\n",
    "    PartialDependenceDisplay.from_estimator(\n",
    "        pipe_logit, X_test, \n",
    "        features=[\"marital-status\", \"capital-gain\", \"education\", (\"marital-status\", \"education\")], \n",
    "        categorical_features=[\"marital-status\", \"education\"], \n",
    "        n_cols=4, ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118753c-28c7-44f4-a71a-45ff1a7b22fc",
   "metadata": {},
   "source": [
    "### Logistic regression with interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fb95a-1fdc-47e2-9ab1-71ce5629ba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import set_visualize_provider\n",
    "from interpret.provider import InlineProvider\n",
    "set_visualize_provider(InlineProvider())\n",
    "\n",
    "from interpret.glassbox import LogisticRegression as ILogisticRegression\n",
    "interpret_logit_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"preprocess\", column_transformer),\n",
    "    (\"logit\", ILogisticRegression(penalty=None, random_state=prng, max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce66ee8-c1c3-4989-b68f-922b040bf691",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_logit_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed65e5a-3ff1-49d0-b0a6-3681a10239b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs = interpret_logit_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score_test = roc_auc_score(y_test, predicted_probs)\n",
    "print(f\"AUC on the test set for interpret logit is {round(auc_score_test, 4)}\")  # same as before as it is only a wrapper around sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d237b-ab14-4e71-93cf-5b6165fdeefb",
   "metadata": {},
   "source": [
    "### Glassbox explainers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d038d81-a5b0-44f4-af09-df8ec1b4ae31",
   "metadata": {},
   "source": [
    "#### Global: coefficient values (marginal contribution to log odds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aaf3f6-e911-4518-9163-f43a2354b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "logit_global = interpret_logit_pipe[\"logit\"].explain_global(name=\"Logit\")\n",
    "show(logit_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52d8f5-313f-437c-8ff3-01719c57ae6b",
   "metadata": {},
   "source": [
    "Unfortunately, interpret's explainers cannot work on Pipelines directly, so we have to transform the test data separately (using the transformation steps of the Pipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed27415-8e56-43f1-af5b-e71a03fb8ef4",
   "metadata": {},
   "source": [
    "#### Local: individual predictions (weight * feature value within sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034cfa0-51b2-4598-b95a-302273152ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_local = interpret_logit_pipe[\"logit\"].explain_local(interpret_logit_pipe[:-1].transform(X_test)[:15], y_test[:15], name=\"Logit\")\n",
    "show(logit_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09cfe5-3845-4b65-9f49-fbe92a5ee8d1",
   "metadata": {},
   "source": [
    "### Explainable Boosting Machine (EBM)\n",
    "\n",
    "EBM is a glassbox model, designed to have accuracy comparable to state-of-the-art machine learning methods like Random Forest and Boosted Trees, while being highly intelligibile and explainable. EBM is a generalized additive model. See [this video](https://www.youtube.com/watch?v=MREiHgHgl0k&ab_channel=MicrosoftDeveloper) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5a7a7-5c90-4f90-95ed-d43dce2382a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# as EBM is based on trees, it can handle categorical features so no need for one-hot-encoding\n",
    "ebm_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"ebm\", ExplainableBoostingClassifier(random_state=20240325))  # cannot accept RandomState directly\n",
    "])\n",
    "ebm_pipe.fit(X_train, y_train)  \n",
    "\n",
    "auc = roc_auc_score(y_test, ebm_pipe.predict_proba(X_test)[:, 1])\n",
    "print(f\"AUC on the test set for EBM is {round(auc, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ddd07-f208-4dbf-9fc5-10cd169f31d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(ebm_pipe[\"ebm\"].explain_global(name=\"EBM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eddca63-2d60-4e36-9de5-186d8b5ef9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(ebm_pipe[\"ebm\"].explain_local(X_test[:15], y_test[:15], name=\"EBM\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d59c8-ac6f-43d5-b078-f3434e2150ca",
   "metadata": {},
   "source": [
    "### Comparison: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de3389-3262-4a00-9345-cf5ab8501e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# sklearn cannot handle categorical features so we need to one-hot-encode\n",
    "rf_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"preprocess\", column_transformer),\n",
    "    (\"rf\", RandomForestClassifier(random_state=20240325))\n",
    "])\n",
    "\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "predicted_probs = rf_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score_test = roc_auc_score(y_test, predicted_probs)\n",
    "print(f\"AUC on the test set for RF is {round(auc_score_test, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f7266-02a1-4d43-9db2-4cc87b0c41f4",
   "metadata": {},
   "source": [
    "### Blackbox explainers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac322b-dc8a-454b-b00a-c048f949565e",
   "metadata": {},
   "source": [
    "#### Some data manipulation to make categorical explanations easier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84672741-933b-470a-8350-5362d2249619",
   "metadata": {},
   "source": [
    "Unfortunately, categorical features are not handled automatically, so we need to do some workarounds.\n",
    "\n",
    "1. Label encode categorical variables so that they are represented by numbers (strings would throw errors).\n",
    "2. Store the categories represented by the coded numeric values for each variable.\n",
    "3. Do one-hot encoding for modelling (but not for explanation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf32b7-2236-475c-a718-a786070a64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding (encode missings as well) & Save category values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_encoded = X\n",
    "category_names = {}\n",
    "for var in categorical_vars:\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(X[var])\n",
    "    X_encoded[var] = label_encoder.transform(X[var])\n",
    "    category_names[var] = label_encoder.classes_\n",
    "\n",
    "category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bab4d-dd94-417c-a232-3c1520a626b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation on the whole data\n",
    "prng = np.random.RandomState(20240325)\n",
    "X_train_encoded, X_test_encoded, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=prng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bad82f-7d2f-4020-8c98-3db0849e1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.iloc[0], X_train_encoded.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8002c14d-4187-47c2-b659-d0792f478453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the model to ensure one-hot-encoded features correspond to the same\n",
    "rf_pipe_encoded = Pipeline([\n",
    "    (\"preprocess\", column_transformer),\n",
    "    (\"rf\", RandomForestClassifier(random_state=20240325))\n",
    "])\n",
    "\n",
    "rf_pipe_encoded.fit(X_train_encoded, y_train)\n",
    "predicted_probs = rf_pipe_encoded.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "auc_score_test = roc_auc_score(y_test, predicted_probs)\n",
    "print(f\"AUC on the test set for encoded RF is {round(auc_score_test, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d33cd-73d2-44e3-b03d-f55b45721775",
   "metadata": {},
   "source": [
    "#### PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414a4c9f-cb9e-4ee6-be21-b71166c4c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import PartialDependence\n",
    "\n",
    "feature_types = [\"nominal\" if v in categorical_vars else \"continuous\" for v in X.columns]\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Found unknown categories*\")\n",
    "    pdp = PartialDependence(\n",
    "        rf_pipe_encoded, X_test_encoded, \n",
    "        feature_names=X.columns.to_list(), \n",
    "        feature_types=feature_types\n",
    "    )\n",
    "\n",
    "show(pdp.explain_global(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb322ee-4bda-4092-be2f-bb9193985fbe",
   "metadata": {},
   "source": [
    "#### Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef578b-47b9-4b58-a3f3-2ef2aba467f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import LimeTabular\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lime = LimeTabular(\n",
    "    rf_pipe_encoded, X_train_encoded,\n",
    "    categorical_features=categorical_vars, \n",
    "    categorical_names=category_names, \n",
    "    kernel_width=3,\n",
    "    random_state=20240325\n",
    ")\n",
    "\n",
    "# LimeTabular expects numeric labels so we need to encode it first\n",
    "y_test_numeric = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", message=\"Found unknown categories*\")\n",
    "    show(lime.explain_local(X_test_encoded[:15], y_test_numeric[:15], name=\"LIME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd2f29-0815-4c19-9119-10b77c8b0cee",
   "metadata": {},
   "source": [
    "#### Shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c95da-0aa4-4fbb-b296-4b65dedc7357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.blackbox import ShapKernel\n",
    "\n",
    "shap_kernel = ShapKernel(rf_pipe_encoded, X_train_encoded.sample(100))\n",
    "show(shap_kernel.explain_local(X_test_encoded[:15], y_test_numeric[:15]), name = \"SHAP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
