{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67z-Nl9CmjL9"
   },
   "source": [
    "## Train a neural network on heart failure data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Recap data preparation from last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2Pcgwh_El_s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# look at heart failure data from last class\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "heart_data = pd.read_csv(\"../data/heart_failure/heart_train.csv\")\n",
    "features = heart_data.drop(columns=[\"HeartDisease\"])\n",
    "label = heart_data[\"HeartDisease\"]\n",
    "\n",
    "prng = np.random.RandomState(20240311)  # ensure we have the same split as in last class\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=prng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate simple logistic regression as a benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# preprocess\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
    "categorical_vars = heart_data.select_dtypes(include=\"object\").columns.to_list()\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [(\"create_dummies\", one_hot_encoder, categorical_vars)],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# pipeline with lgoit\n",
    "pipe_logit = Pipeline([\n",
    "    (\"preprocess\", column_transformer),\n",
    "    (\"scale\", MinMaxScaler()),\n",
    "    (\"logit\", LogisticRegression(penalty=None))\n",
    "])\n",
    "pipe_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the performance by AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "predicted_probs = pipe_logit.predict_proba(X_test)[:, 1]\n",
    "auc_score_test = roc_auc_score(y_test, predicted_probs)\n",
    "print(f\"AUC on the test set for simple logit is {round(auc_score_test, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate a neural network on the heart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "pipe_mlp = Pipeline([\n",
    "    (\"preprocess\", column_transformer),\n",
    "    (\"scale\", MinMaxScaler()),\n",
    "    (\"MLP\", MLPClassifier(random_state = prng))\n",
    "])\n",
    "pipe_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mlp[\"MLP\"].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs_mlp = pipe_mlp.predict_proba(X_test)[:, 1]\n",
    "auc_score_test_mlp = roc_auc_score(y_test, predicted_probs_mlp)\n",
    "print(f\"AUC on the test set for basic MLP is {round(auc_score_test_mlp, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nSRp7AzmpiO"
   },
   "source": [
    "## Image recognition with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnojMGQDvjTA"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rRY47k5mtUs",
    "outputId": "c4d28f97-e11a-4282-d773-a169b93eb11f"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Look at the dimensions\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we work here with numpy arrays instead of pandas dataframes\n",
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OwzQuuVsoxxj",
    "outputId": "ccecfb6a-b4ba-4295-b2da-677f0136d008"
   },
   "outputs": [],
   "source": [
    "# Visualize some items in a grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(10,10))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(X_train[i], cmap=\"binary\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5JQHyLpwV0s",
    "outputId": "d40c0413-150a-48b2-c53c-6f48999bd4d0"
   },
   "outputs": [],
   "source": [
    "X_train[0,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Qi6c3yzyLoY",
    "outputId": "cf49ab71-5387-4c12-c9fe-4e897ac0c296"
   },
   "outputs": [],
   "source": [
    "# intentionally choose a small train set to decrease computational burden\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.8, random_state=prng)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_val:  {X_val.shape}\")\n",
    "print(f\"y_val:  {y_val.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67Va4vk8vlaZ"
   },
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohNTotmCprsN",
    "outputId": "1e659650-d794-4321-bd9d-4211a5917c4e"
   },
   "outputs": [],
   "source": [
    "# Benchmark #1 (silly):\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mode\n",
    "\n",
    "most_frequent = mode(y_train)\n",
    "print(f\"Most frequent element is: {most_frequent}\")\n",
    "accuracy_most_frequent = accuracy_score(y_val, np.repeat(most_frequent, len(y_val)))\n",
    "print(f\"Accuracy for our no-brainer model: {round(accuracy_most_frequent, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a state-of-the-art machine learning model almost as easily. It can be used as our second benchmark. Note that our train data is multidimensional, instead of having `k` features, we have `ixj` features. In order to apply a standard ML algorithm, we need to flatten our data first. \n",
    "\n",
    "Since we need to apply the flattening to both the train and the test data, a clever way to do this is to build this step into the pipeline. Since we are applying the transformation to the whole feature matrix rather than specific columns, we can use the `FunctionTransformer`, which can apply any user-defined function to the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJLriIz-pv5_",
    "outputId": "c36ada3c-9217-4054-8af8-6767911d5565"
   },
   "outputs": [],
   "source": [
    "# Benchmark #2 (RF):\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def flatten_data(X):\n",
    "    return X.reshape(X.shape[0], -1)\n",
    "\n",
    "# TODO:\n",
    "# 1. create pipeline with two steps: flatten & model\n",
    "# 2. fit the pipeline\n",
    "# 3. get predicted classes for the validation set\n",
    "# 4. calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "Tk68jJDKsNuB",
    "outputId": "1107f28e-1b69-447a-f4f9-eb0ad131dbd5"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_val, predictions_rf)\n",
    "\n",
    "# # visualize with seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7yecmMAvpRr"
   },
   "source": [
    "### Sklearn MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olhTLDa4uGSq",
    "outputId": "c14b2193-dbf3-4824-f3f0-15f9d78dfc91"
   },
   "outputs": [],
   "source": [
    "def scale_features(X):\n",
    "    return X / 255\n",
    "\n",
    "# TODO:\n",
    "# 1. create pipeline with three steps: flatten & scale & model\n",
    "# 2. fit the pipeline\n",
    "# 3. get predicted classes for the validation set\n",
    "# 4. calculate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FezgFc-Svr10"
   },
   "source": [
    "### Keras: simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `scikit-learn` provides a wide range of machine learning algorithms, `keras` is specifically designed for building and training neural networks and deep learning models, making it more suitable for tasks involving complex patterns and large datasets. It offers several advanced features that `sklearn` does not, such as training networks with complex architectures (such as convolutional neural networks), or applying pre-processing techniques common to deep learning (such as scaling and flattening), or the ability to transfer learning from pre-trained networks.\n",
    "\n",
    "Keras is a high-level neural network API that provides a simple and intuitive interface for building and training deep learning models. Keras can run on multiple backends, with TensorFlowTensor becoming the default. TensorFlow is a standalone, low-level deep learning library developed by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import __version__ as keras_version\n",
    "print(keras_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-xypJUNs0I1",
    "outputId": "16bd62fe-bcf8-493b-d111-82a6e6ce6062"
   },
   "outputs": [],
   "source": [
    "# Convert target variables to categorical\n",
    "num_classes = 10\n",
    "y_sets = [y_train, y_test, y_val]\n",
    "y_train, y_test, y_val = [to_categorical(y, num_classes=num_classes) for y in y_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Flatten, Rescaling, Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=X_train.shape[1:]), \n",
    "    Flatten(), \n",
    "    Rescaling(1./255), \n",
    "    Dense(100, activation='relu'), \n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "# 784*100+100\n",
    "# 100*10+10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of the backend is indeed constructed around **tensors**. Tensors are akin to NumPy arrays, as they hold numerical values of different dimensions. What sets them apart is their specialized role in deep learning: they are equipped with built-in gradient computation, seamlessly integrate within a computational graph, and are capable of leveraging hardware accelerators like TPUs and GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[3].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wg7ED4Nzstsj",
    "outputId": "323eef39-f193-4eb9-d307-b43036c4720c"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "keras.utils.set_random_seed(20240318)  # for reproducibility\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCU5Et156F56",
    "outputId": "a861f3de-f4ed-4001-bbab-3cc93aaa2fc1"
   },
   "outputs": [],
   "source": [
    "# Evaluation of the model on the validation set\n",
    "scores = model.evaluate(X_val, y_val)\n",
    "print(f\"Accuracy for keras MLP: {round(scores[1], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(fit_history):\n",
    "    plt.plot(fit_history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(fit_history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the maximum is not yet found. Let's train the network a little bit longer. Note that unless we recreate the model, the process starts from the point where it previously ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run for at least 50 epochs, store the result in history_longer_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_history = {\n",
    "    \"accuracy\": history.history[\"accuracy\"] + history_longer_train.history[\"accuracy\"],\n",
    "    \"val_accuracy\": history.history[\"val_accuracy\"] + history_longer_train.history[\"val_accuracy\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(total_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import clone_model\n",
    "\n",
    "# to make sure the process starts over, we need to create a new model instanse and compile it\n",
    "cloned_model = clone_model(model)\n",
    "cloned_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit with early stopping\n",
    "history_with_early_stopping = cloned_model.fit(\n",
    "    X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=512, \n",
    "    callbacks=[EarlyStopping(monitor='val_accuracy')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_with_early_stopping.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Build the model\n",
    "regularized_model = Sequential([\n",
    "    Input(shape=X_train.shape[1:]),\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "regularized_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(regularized_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_regularized = regularized_model.fit(\n",
    "    X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=512,\n",
    "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=2)] # two epochs without any improvement is still fine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_regularized.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras: Deep network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "deep_model = Sequential([    \n",
    "    Input(shape=X_train.shape[1:]),\n",
    "    Flatten(),\n",
    "    Rescaling(1./255),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the deep_model\n",
    "deep_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(deep_model.summary())\n",
    "# 784*256+256\n",
    "# 256*256+256\n",
    "# 256*10+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_deep = deep_model.fit(\n",
    "    X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=512,\n",
    "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_deep.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4l_R89ZzA7q"
   },
   "source": [
    "### Convolution with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers are designed to operate on images or time-series data with multiple sensors, so they expect input data to have a channel dimension. In our case, since we are dealing with grayscale images, we will have only one channel (unlike the RGB channels in color images). To ensure our data has this channel dimension, we need to reshape our data first. Let's collect all the preprocessing steps, and create new preprocessed sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape\n",
    "\n",
    "preprocess = Sequential([\n",
    "    Reshape(target_shape=(X_train.shape[1], X_train.shape[2], 1)),  # explicitly state the 4th (channel) dimension\n",
    "    Rescaling(1./255)\n",
    "])\n",
    "\n",
    "X_sets = [X_train, X_test, X_val]\n",
    "X_train_4D, X_test_4D, X_val_4D = [preprocess(X) for X in X_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_cK0Nkay9I7",
    "outputId": "d57c03e3-fdd9-455f-e653-090693d79721"
   },
   "outputs": [],
   "source": [
    "X_train_4D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# Build the model\n",
    "model_cnn = Sequential([\n",
    "    Input(shape=X_train_4D.shape[1:]),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_cnn.summary())\n",
    "# 32*(3*3+1)\n",
    "# 64*(32*3*3+1) -- bias term is not channel-specific as it does not depend on the input data\n",
    "# (64*5*5)*10+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YtyWJ3Y7lXv"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history_cnn = model_cnn.fit(\n",
    "    X_train_4D, y_train, validation_data=(X_val_4D, y_val), epochs=50, batch_size=512,\n",
    "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_cnn.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72ToIajE7pGe"
   },
   "source": [
    "### Other useful tricks: data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RandomRotation, RandomZoom\n",
    "\n",
    "# TODO: Create a new Sequential model called data augmentation with two layers: \n",
    "    # RandomRotation(0.1, fill_mode=\"nearest\") & RandomZoom(0.2, fill_mode=\"nearest\")\n",
    "\n",
    "# Look at the transformations\n",
    "number_of_digits_to_show = 5\n",
    "fig, axs = plt.subplots(2, number_of_digits_to_show, figsize=(2*number_of_digits_to_show, 4))\n",
    "for _ in range(number_of_digits_to_show):\n",
    "    axs[0, _].imshow(X_train_4D[_], cmap=\"binary\")\n",
    "    axs[0, _].axis(\"off\")\n",
    "    axs[1, _].imshow(data_augmentation(X_train_4D[_]), cmap=\"binary\")\n",
    "    axs[1, _].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_da = Sequential([\n",
    "    data_augmentation,\n",
    "    model_cnn\n",
    "])\n",
    "\n",
    "print(model_cnn_da.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_cnn_da.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_cnn_da = model_cnn_da.fit(\n",
    "    X_train_4D, y_train, validation_data=(X_val_4D, y_val), epochs=50, batch_size=512,\n",
    "    callbacks=[EarlyStopping(monitor='val_accuracy', patience=5)] # increase patience for slower learning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our final model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import load_img, img_to_array\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "pretrained_model = ResNet50(weights='imagenet')\n",
    "print(pretrained_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = #TODO\n",
    "img = load_img(img_path, target_size=(224, 224))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = img_to_array(img)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure it is one sample with 3 dimensions\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pretrained_model.predict(x)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import decode_predictions\n",
    "decoded_preds = decode_predictions(preds, top=3)\n",
    "[(i[1], round(i[2], 4)) for i in decoded_preds[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fine-tune these pre-trained models for our purposes by modifying the last few layers, and learn only the new parameters on new data (freezing the weights of the original network). See e.g [this tutorial](https://pyimagesearch.com/2020/04/27/fine-tuning-resnet-with-keras-tensorflow-and-deep-learning/) for more details."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
