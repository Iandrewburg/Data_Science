{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c920e6-3761-4d94-9558-ae995ffa39ac",
   "metadata": {},
   "source": [
    "# <center> Kaggle Competition Assignment <center> Ian Brandenburg (2304791) <center> [GitHub Repo](https://github.com/Iandrewburg/Data_Science/tree/main/Data_Science_2/Assignments/Take_Home_Final)\n",
    "    \n",
    "    \n",
    "The Kaggle competition has been launched, please register using this [link](https://www.kaggle.com/t/f79b637ede074e70a233661b4614083c).\n",
    "\n",
    "You will find the training and test data in the data section of the competition, along with a description of the features. You will need to build models on the training data and make predictions on the test data and submit your solutions to Kaggle. You will also find a sample solution file in the data section that shows the format you will need to use for your own submissions.\n",
    "\n",
    "The deadline for Kaggle solutions is 8PM on 19 April. You will be graded primarily on the basis of your work and how clearly you explain your methods and results. Those in the top three in the competition will receive some extra points. I expect you to experiment with all the methods we have covered: linear models, random forest, gradient boosting, neural networks + parameter tuning, feature engineering.\n",
    "\n",
    "You will see the public score of your best model on the leaderboard. A private dataset will be used to evaluate the final performance of your model to avoid overfitting based on the leaderboard.\n",
    "\n",
    "You should also submit to Moodle the documentation (ipynb and pdf) of your work, including exploratory data analysis, data cleaning, parameter tuning and evaluation. Aim for concise explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156211d-6705-4669-9a65-835e9a896f55",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98f6fb47-2ef1-43d6-94ec-a6482b8525dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3e2f2-32fb-405c-909f-ed9b42a5b7fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Wrangling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044cd425-2b1e-4019-91d4-976186fb8854",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30844dd-201d-416c-80dc-e0040f99a95a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>is_popular</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>594</td>\n",
       "      <td>9</td>\n",
       "      <td>702</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.620438</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.153395</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346</td>\n",
       "      <td>8</td>\n",
       "      <td>1197</td>\n",
       "      <td>0.470143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666209</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.308167</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>484</td>\n",
       "      <td>9</td>\n",
       "      <td>214</td>\n",
       "      <td>0.618090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>639</td>\n",
       "      <td>8</td>\n",
       "      <td>249</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.664740</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177</td>\n",
       "      <td>12</td>\n",
       "      <td>1219</td>\n",
       "      <td>0.397841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.583578</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.441111</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0        594               9               702         0.454545   \n",
       "1        346               8              1197         0.470143   \n",
       "2        484               9               214         0.618090   \n",
       "3        639               8               249         0.621951   \n",
       "4        177              12              1219         0.397841   \n",
       "\n",
       "   n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0               1.0                  0.620438         11               2   \n",
       "1               1.0                  0.666209         21               6   \n",
       "2               1.0                  0.748092          5               2   \n",
       "3               1.0                  0.664740         16               5   \n",
       "4               1.0                  0.583578         21               1   \n",
       "\n",
       "   num_imgs  num_videos  ...  max_positive_polarity  avg_negative_polarity  \\\n",
       "0         1           0  ...               1.000000              -0.153395   \n",
       "1         2          13  ...               1.000000              -0.308167   \n",
       "2         1           0  ...               0.433333              -0.141667   \n",
       "3         8           0  ...               0.500000              -0.500000   \n",
       "4         1           2  ...               0.800000              -0.441111   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                   -0.4                  -0.10                 0.0   \n",
       "1                   -1.0                  -0.10                 0.0   \n",
       "2                   -0.2                  -0.05                 0.0   \n",
       "3                   -0.8                  -0.40                 0.0   \n",
       "4                   -1.0                  -0.05                 0.0   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                       0.0                     0.5   \n",
       "1                       0.0                     0.5   \n",
       "2                       0.0                     0.5   \n",
       "3                       0.0                     0.5   \n",
       "4                       0.0                     0.5   \n",
       "\n",
       "   abs_title_sentiment_polarity  is_popular  article_id  \n",
       "0                           0.0           0           1  \n",
       "1                           0.0           0           3  \n",
       "2                           0.0           0           5  \n",
       "3                           0.0           0           6  \n",
       "4                           0.0           0           7  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"https://raw.githubusercontent.com/Iandrewburg/Data_Science/main/Data_Science_2/Assignments/Take_Home_Final/train.csv\")\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bbc1b2-429b-465f-9e29-d83e646b31cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134</td>\n",
       "      <td>11</td>\n",
       "      <td>217</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818966</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.170370</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>415</td>\n",
       "      <td>11</td>\n",
       "      <td>1041</td>\n",
       "      <td>0.489423</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700321</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.426268</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>625</td>\n",
       "      <td>9</td>\n",
       "      <td>486</td>\n",
       "      <td>0.599585</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.387821</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148</td>\n",
       "      <td>14</td>\n",
       "      <td>505</td>\n",
       "      <td>0.509018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718861</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.284722</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>294</td>\n",
       "      <td>14</td>\n",
       "      <td>274</td>\n",
       "      <td>0.620301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0        134              11               217         0.631579   \n",
       "1        415              11              1041         0.489423   \n",
       "2        625               9               486         0.599585   \n",
       "3        148              14               505         0.509018   \n",
       "4        294              14               274         0.620301   \n",
       "\n",
       "   n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0               1.0                  0.818966          4               2   \n",
       "1               1.0                  0.700321         22               3   \n",
       "2               1.0                  0.727273          4               3   \n",
       "3               1.0                  0.718861          8               4   \n",
       "4               1.0                  0.726190          5               1   \n",
       "\n",
       "   num_imgs  num_videos  ...  min_positive_polarity  max_positive_polarity  \\\n",
       "0         2           0  ...               0.136364                    0.5   \n",
       "1         0          14  ...               0.050000                    1.0   \n",
       "2         1           0  ...               0.062500                    0.7   \n",
       "3         1           1  ...               0.100000                    1.0   \n",
       "4         1           0  ...               0.100000                    0.6   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.170370              -0.200000              -0.155556   \n",
       "1              -0.426268              -1.000000              -0.100000   \n",
       "2              -0.387821              -1.000000              -0.050000   \n",
       "3              -0.284722              -0.400000              -0.050000   \n",
       "4              -0.333333              -0.333333              -0.333333   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.288889                 -0.155556                0.211111   \n",
       "1            0.975000                  0.300000                0.475000   \n",
       "2            0.000000                  0.000000                0.500000   \n",
       "3            0.000000                  0.000000                0.500000   \n",
       "4            0.000000                  0.000000                0.500000   \n",
       "\n",
       "   abs_title_sentiment_polarity  article_id  \n",
       "0                      0.155556           2  \n",
       "1                      0.300000           4  \n",
       "2                      0.000000          10  \n",
       "3                      0.000000          13  \n",
       "4                      0.000000          26  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"https://raw.githubusercontent.com/Iandrewburg/Data_Science/main/Data_Science_2/Assignments/Take_Home_Final/test.csv\")\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdaeb309-5690-43a6-a411-a7a56c7aa3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
       "       'num_keywords', 'data_channel_is_lifestyle',\n",
       "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
       "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
       "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
       "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
       "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
       "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
       "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
       "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
       "       'max_positive_polarity', 'avg_negative_polarity',\n",
       "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'article_id', 'n_tokens_title_squared',\n",
       "       'n_tokens_content_squared', 'n_unique_tokens_squared',\n",
       "       'n_non_stop_words_squared', 'n_non_stop_unique_tokens_squared',\n",
       "       'average_token_length_squared', 'num_keywords_squared',\n",
       "       'title_subjectivity_squared', 'title_sentiment_polarity_squared',\n",
       "       'abs_title_subjectivity_squared',\n",
       "       'abs_title_sentiment_polarity_squared',\n",
       "       'n_tokens_title_n_tokens_content_interaction',\n",
       "       'n_tokens_title_n_unique_tokens_interaction',\n",
       "       'n_tokens_title_n_non_stop_words_interaction',\n",
       "       'n_tokens_title_n_non_stop_unique_tokens_interaction',\n",
       "       'n_tokens_title_average_token_length_interaction',\n",
       "       'n_tokens_title_num_keywords_interaction',\n",
       "       'n_tokens_content_n_unique_tokens_interaction',\n",
       "       'n_tokens_content_n_non_stop_words_interaction',\n",
       "       'n_tokens_content_n_non_stop_unique_tokens_interaction',\n",
       "       'n_tokens_content_average_token_length_interaction',\n",
       "       'n_tokens_content_num_keywords_interaction',\n",
       "       'n_unique_tokens_n_non_stop_words_interaction',\n",
       "       'n_unique_tokens_n_non_stop_unique_tokens_interaction',\n",
       "       'n_unique_tokens_average_token_length_interaction',\n",
       "       'n_unique_tokens_num_keywords_interaction',\n",
       "       'n_non_stop_words_n_non_stop_unique_tokens_interaction',\n",
       "       'n_non_stop_words_average_token_length_interaction',\n",
       "       'n_non_stop_words_num_keywords_interaction',\n",
       "       'n_non_stop_unique_tokens_average_token_length_interaction',\n",
       "       'n_non_stop_unique_tokens_num_keywords_interaction',\n",
       "       'average_token_length_num_keywords_interaction',\n",
       "       'title_subjectivity_title_sentiment_polarity_interaction',\n",
       "       'title_subjectivity_abs_title_subjectivity_interaction',\n",
       "       'title_subjectivity_abs_title_sentiment_polarity_interaction',\n",
       "       'title_sentiment_polarity_abs_title_subjectivity_interaction',\n",
       "       'title_sentiment_polarity_abs_title_sentiment_polarity_interaction',\n",
       "       'abs_title_subjectivity_abs_title_sentiment_polarity_interaction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced915d4-89b8-49e9-a4a2-84e12f0aa288",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854cf70-2030-4ae0-bc31-b84b3c68f513",
   "metadata": {},
   "source": [
    "### Variable Descriptions\n",
    "---\n",
    "\n",
    "\n",
    "    timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
    "    n_tokens_title: Number of words in the title\n",
    "    n_tokens_content: Number of words in the content\n",
    "    n_unique_tokens: Rate of unique words in the content\n",
    "    n_non_stop_words: Rate of non-stop words in the content\n",
    "    n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "    num_hrefs: Number of links\n",
    "    num_self_hrefs: Number of links to other articles published by Mashable\n",
    "    num_imgs: Number of images\n",
    "    num_videos: Number of videos\n",
    "    average_token_length: Average length of the words in the content\n",
    "    num_keywords: Number of keywords in the metadata\n",
    "    data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
    "    data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
    "    data_channel_is_bus: Is data channel 'Business'?\n",
    "    data_channel_is_socmed: Is data channel 'Social Media'?\n",
    "    data_channel_is_tech: Is data channel 'Tech'?\n",
    "    data_channel_is_world: Is data channel 'World'?\n",
    "    kw_min_min: Worst keyword (min. shares)\n",
    "    kw_max_min: Worst keyword (max. shares)\n",
    "    kw_avg_min: Worst keyword (avg. shares)\n",
    "    kw_min_max: Best keyword (min. shares)\n",
    "    kw_max_max: Best keyword (max. shares)\n",
    "    kw_avg_max: Best keyword (avg. shares)\n",
    "    kw_min_avg: Avg. keyword (min. shares)\n",
    "    kw_max_avg: Avg. keyword (max. shares)\n",
    "    kw_avg_avg: Avg. keyword (avg. shares)\n",
    "    self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
    "    self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
    "    self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
    "    weekday_is_monday: Was the article published on a Monday?\n",
    "    weekday_is_tuesday: Was the article published on a Tuesday?\n",
    "    weekday_is_wednesday: Was the article published on a Wednesday?\n",
    "    weekday_is_thursday: Was the article published on a Thursday?\n",
    "    weekday_is_friday: Was the article published on a Friday?\n",
    "    weekday_is_saturday: Was the article published on a Saturday?\n",
    "    weekday_is_sunday: Was the article published on a Sunday?\n",
    "    is_weekend: Was the article published on the weekend?\n",
    "    LDA_00: Closeness to LDA topic 0\n",
    "    LDA_01: Closeness to LDA topic 1\n",
    "    LDA_02: Closeness to LDA topic 2\n",
    "    LDA_03: Closeness to LDA topic 3\n",
    "    LDA_04: Closeness to LDA topic 4\n",
    "    global_subjectivity: Text subjectivity\n",
    "    global_sentiment_polarity: Text sentiment polarity\n",
    "    global_rate_positive_words: Rate of positive words in the content\n",
    "    global_rate_negative_words: Rate of negative words in the content\n",
    "    rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "    rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "    avg_positive_polarity: Avg. polarity of positive words\n",
    "    min_positive_polarity: Min. polarity of positive words\n",
    "    max_positive_polarity: Max. polarity of positive words\n",
    "    avg_negative_polarity: Avg. polarity of negative words\n",
    "    min_negative_polarity: Min. polarity of negative words\n",
    "    max_negative_polarity: Max. polarity of negative words\n",
    "    title_subjectivity: Title subjectivity\n",
    "    title_sentiment_polarity: Title polarity\n",
    "    abs_title_subjectivity: Absolute subjectivity level\n",
    "    abs_title_sentiment_polarity: Absolute polarity level\n",
    "    is_popular: Whether or not the article was among the most popular ones based on shares on social media\n",
    "    article_id: Unique identifier of the article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "942c303a-6a91-42f4-bfa2-4e1508a1a038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>is_popular</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>355.645646</td>\n",
       "      <td>10.390812</td>\n",
       "      <td>545.008274</td>\n",
       "      <td>0.555076</td>\n",
       "      <td>1.005852</td>\n",
       "      <td>0.695432</td>\n",
       "      <td>10.912690</td>\n",
       "      <td>3.290788</td>\n",
       "      <td>4.524535</td>\n",
       "      <td>1.263546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757780</td>\n",
       "      <td>-0.259709</td>\n",
       "      <td>-0.520981</td>\n",
       "      <td>-0.107793</td>\n",
       "      <td>0.281878</td>\n",
       "      <td>0.069691</td>\n",
       "      <td>0.341427</td>\n",
       "      <td>0.155234</td>\n",
       "      <td>0.121649</td>\n",
       "      <td>19834.913530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>214.288261</td>\n",
       "      <td>2.110135</td>\n",
       "      <td>469.358037</td>\n",
       "      <td>4.064572</td>\n",
       "      <td>6.039655</td>\n",
       "      <td>3.768796</td>\n",
       "      <td>11.316508</td>\n",
       "      <td>3.840874</td>\n",
       "      <td>8.213823</td>\n",
       "      <td>4.189080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247293</td>\n",
       "      <td>0.128488</td>\n",
       "      <td>0.290454</td>\n",
       "      <td>0.095672</td>\n",
       "      <td>0.323461</td>\n",
       "      <td>0.264379</td>\n",
       "      <td>0.188735</td>\n",
       "      <td>0.225066</td>\n",
       "      <td>0.326886</td>\n",
       "      <td>11432.376037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.626126</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328704</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9965.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>342.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>0.539894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690566</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.252827</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19859.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>545.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.755208</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186494</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29742.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>1042.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39643.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "count  29733.000000    29733.000000      29733.000000     29733.000000   \n",
       "mean     355.645646       10.390812        545.008274         0.555076   \n",
       "std      214.288261        2.110135        469.358037         4.064572   \n",
       "min        8.000000        2.000000          0.000000         0.000000   \n",
       "25%      164.000000        9.000000        246.000000         0.471400   \n",
       "50%      342.000000       10.000000        409.000000         0.539894   \n",
       "75%      545.000000       12.000000        712.000000         0.609375   \n",
       "max      731.000000       23.000000       8474.000000       701.000000   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens     num_hrefs  \\\n",
       "count      29733.000000              29733.000000  29733.000000   \n",
       "mean           1.005852                  0.695432     10.912690   \n",
       "std            6.039655                  3.768796     11.316508   \n",
       "min            0.000000                  0.000000      0.000000   \n",
       "25%            1.000000                  0.626126      4.000000   \n",
       "50%            1.000000                  0.690566      8.000000   \n",
       "75%            1.000000                  0.755208     14.000000   \n",
       "max         1042.000000                650.000000    304.000000   \n",
       "\n",
       "       num_self_hrefs      num_imgs    num_videos  ...  max_positive_polarity  \\\n",
       "count    29733.000000  29733.000000  29733.000000  ...           29733.000000   \n",
       "mean         3.290788      4.524535      1.263546  ...               0.757780   \n",
       "std          3.840874      8.213823      4.189080  ...               0.247293   \n",
       "min          0.000000      0.000000      0.000000  ...               0.000000   \n",
       "25%          1.000000      1.000000      0.000000  ...               0.600000   \n",
       "50%          2.000000      1.000000      0.000000  ...               0.800000   \n",
       "75%          4.000000      4.000000      1.000000  ...               1.000000   \n",
       "max         74.000000    111.000000     91.000000  ...               1.000000   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "count           29733.000000           29733.000000           29733.000000   \n",
       "mean               -0.259709              -0.520981              -0.107793   \n",
       "std                 0.128488               0.290454               0.095672   \n",
       "min                -1.000000              -1.000000              -1.000000   \n",
       "25%                -0.328704              -0.700000              -0.125000   \n",
       "50%                -0.252827              -0.500000              -0.100000   \n",
       "75%                -0.186494              -0.300000              -0.050000   \n",
       "max                 0.000000               0.000000               0.000000   \n",
       "\n",
       "       title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "count        29733.000000              29733.000000            29733.000000   \n",
       "mean             0.281878                  0.069691                0.341427   \n",
       "std              0.323461                  0.264379                0.188735   \n",
       "min              0.000000                 -1.000000                0.000000   \n",
       "25%              0.000000                  0.000000                0.166667   \n",
       "50%              0.144444                  0.000000                0.500000   \n",
       "75%              0.500000                  0.136364                0.500000   \n",
       "max              1.000000                  1.000000                0.500000   \n",
       "\n",
       "       abs_title_sentiment_polarity    is_popular    article_id  \n",
       "count                  29733.000000  29733.000000  29733.000000  \n",
       "mean                       0.155234      0.121649  19834.913530  \n",
       "std                        0.225066      0.326886  11432.376037  \n",
       "min                        0.000000      0.000000      1.000000  \n",
       "25%                        0.000000      0.000000   9965.000000  \n",
       "50%                        0.000000      0.000000  19859.000000  \n",
       "75%                        0.250000      0.000000  29742.000000  \n",
       "max                        1.000000      1.000000  39643.000000  \n",
       "\n",
       "[8 rows x 61 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b07d4d81-512a-4e49-9274-457839f87f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is 29733 rows, and 61 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the training set is {train_data.shape[0]} rows, and {train_data.shape[1]} columns.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2e62837-bcf9-4f77-9f5d-cc532454a9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 0 missing values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "total_missing_values = train_data.isnull().sum()[train_data.isnull().sum() > 0].sum()\n",
    "print(f\"There are a total of {total_missing_values} missing values in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a44f9aa-7c8e-4556-831c-3542bffe08a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
       "       'num_keywords', 'data_channel_is_lifestyle',\n",
       "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
       "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
       "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
       "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
       "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
       "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
       "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
       "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
       "       'max_positive_polarity', 'avg_negative_polarity',\n",
       "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'is_popular', 'article_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fade411-c1c4-4598-8e61-0c8d2ea9a087",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54056a3b-786d-45e9-bbc5-df243acf9c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining variable groups\n",
    "basic_text_features = ['n_tokens_title',\n",
    "                       'n_tokens_content',\n",
    "                       'n_unique_tokens',\n",
    "                       'n_non_stop_words',\n",
    "                       'n_non_stop_unique_tokens',\n",
    "                       'average_token_length',\n",
    "                       'num_keywords']\n",
    "content_properties = ['num_hrefs',\n",
    "                      'num_self_hrefs',\n",
    "                      'num_imgs',\n",
    "                      'num_videos',\n",
    "                      'global_subjectivity',\n",
    "                      'global_sentiment_polarity',\n",
    "                      'global_rate_positive_words',\n",
    "                      'global_rate_negative_words']\n",
    "keyword_performance = ['kw_min_min',\n",
    "                       'kw_max_min',\n",
    "                       'kw_avg_min',\n",
    "                       'kw_min_max',\n",
    "                       'kw_max_max',\n",
    "                       'kw_avg_max',\n",
    "                       'kw_min_avg',\n",
    "                       'kw_max_avg',\n",
    "                       'kw_avg_avg']\n",
    "self_reference_metrics = ['self_reference_min_shares',\n",
    "                          'self_reference_max_shares',\n",
    "                          'self_reference_avg_sharess']\n",
    "publication_timing = ['weekday_is_monday',\n",
    "                      'weekday_is_tuesday',\n",
    "                      'weekday_is_wednesday',\n",
    "                      'weekday_is_thursday',\n",
    "                      'weekday_is_friday',\n",
    "                      'weekday_is_saturday',\n",
    "                      'weekday_is_sunday',\n",
    "                      'is_weekend']\n",
    "content_topic_and_sentiment = ['data_channel_is_lifestyle',\n",
    "                               'data_channel_is_entertainment',\n",
    "                               'data_channel_is_bus',\n",
    "                               'data_channel_is_socmed',\n",
    "                               'data_channel_is_tech',\n",
    "                               'data_channel_is_world',\n",
    "                               'LDA_00',\n",
    "                               'LDA_01',\n",
    "                               'LDA_02',\n",
    "                               'LDA_03',\n",
    "                               'LDA_04',\n",
    "                               'rate_positive_words',\n",
    "                               'rate_negative_words',\n",
    "                               'avg_positive_polarity',\n",
    "                               'min_positive_polarity', \n",
    "                               'max_positive_polarity',\n",
    "                               'avg_negative_polarity',\n",
    "                               'min_negative_polarity',\n",
    "                               'max_negative_polarity']\n",
    "title_sentiment = ['title_subjectivity',\n",
    "                   'title_sentiment_polarity',\n",
    "                   'abs_title_subjectivity',\n",
    "                   'abs_title_sentiment_polarity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbf2eb71-32a4-4a62-8c66-9331de8e3cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################SQUARED TERMS###################\n",
    "# square basic features in the training set\n",
    "for var in basic_text_features:\n",
    "    train_data[f'{var}_squared'] = train_data[var] ** 2\n",
    "\n",
    "# square basic features in the test set\n",
    "for var in basic_text_features:\n",
    "    test_data[f'{var}_squared'] = test_data[var] ** 2\n",
    "    \n",
    "# square title sentiment features in the training set\n",
    "for var in title_sentiment:\n",
    "    train_data[f'{var}_squared'] = train_data[var] ** 2\n",
    "\n",
    "# square title sentiment features in the test set\n",
    "for var in title_sentiment:\n",
    "    test_data[f'{var}_squared'] = test_data[var] ** 2\n",
    "    \n",
    "################INTERACTION TERMS##################\n",
    "# Interacting the basic features\n",
    "for (var1, var2) in combinations(basic_text_features, 2):\n",
    "    train_data[f'{var1}_{var2}_interaction'] = train_data[var1] * train_data[var2]\n",
    "    \n",
    "for (var1, var2) in combinations(basic_text_features, 2):\n",
    "    test_data[f'{var1}_{var2}_interaction'] = test_data[var1] * test_data[var2]\n",
    "\n",
    "# Interacting the title sentiment features\n",
    "for (var1, var2) in combinations(title_sentiment, 2):\n",
    "    train_data[f'{var1}_{var2}_interaction'] = train_data[var1] * train_data[var2]\n",
    "    \n",
    "for (var1, var2) in combinations(title_sentiment, 2):\n",
    "    test_data[f'{var1}_{var2}_interaction'] = test_data[var1] * test_data[var2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9619686a-3e67-4b51-a78c-19aded667402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sqrd_title_sentiment = ['title_subjectivity_squared',\n",
    "                        'title_sentiment_polarity_squared',\n",
    "                        'abs_title_subjectivity_squared',\n",
    "                        'abs_title_sentiment_polarity_squared']\n",
    "sqrd_basic_text_features = ['n_tokens_title_squared',\n",
    "                            'n_tokens_content_squared',\n",
    "                            'n_unique_tokens_squared',\n",
    "                            'n_non_stop_words_squared',\n",
    "                            'n_non_stop_unique_tokens_squared',\n",
    "                            'average_token_length_squared',\n",
    "                            'num_keywords_squared']\n",
    "interaction_basic_text_features = ['n_tokens_title_n_tokens_content_interaction',\n",
    "                                  'n_tokens_title_n_unique_tokens_interaction',\n",
    "                                  'n_tokens_title_n_non_stop_words_interaction',\n",
    "                                  'n_tokens_title_n_non_stop_unique_tokens_interaction',\n",
    "                                  'n_tokens_title_average_token_length_interaction',\n",
    "                                  'n_tokens_title_num_keywords_interaction',\n",
    "                                  'n_tokens_content_n_unique_tokens_interaction',\n",
    "                                  'n_tokens_content_n_non_stop_words_interaction',\n",
    "                                  'n_tokens_content_n_non_stop_unique_tokens_interaction',\n",
    "                                  'n_tokens_content_average_token_length_interaction',\n",
    "                                  'n_tokens_content_num_keywords_interaction',\n",
    "                                  'n_unique_tokens_n_non_stop_words_interaction',\n",
    "                                  'n_unique_tokens_n_non_stop_unique_tokens_interaction',\n",
    "                                  'n_unique_tokens_average_token_length_interaction',\n",
    "                                  'n_unique_tokens_num_keywords_interaction',\n",
    "                                  'n_non_stop_words_n_non_stop_unique_tokens_interaction',\n",
    "                                  'n_non_stop_words_average_token_length_interaction',\n",
    "                                  'n_non_stop_words_num_keywords_interaction',\n",
    "                                  'n_non_stop_unique_tokens_average_token_length_interaction',\n",
    "                                  'n_non_stop_unique_tokens_num_keywords_interaction',\n",
    "                                  'average_token_length_num_keywords_interaction']\n",
    "interaction_title_sentiment_features = ['title_subjectivity_title_sentiment_polarity_interaction',\n",
    "                                       'title_subjectivity_abs_title_subjectivity_interaction',\n",
    "                                       'title_subjectivity_abs_title_sentiment_polarity_interaction',\n",
    "                                       'title_sentiment_polarity_abs_title_subjectivity_interaction',\n",
    "                                       'title_sentiment_polarity_abs_title_sentiment_polarity_interaction',\n",
    "                                       'abs_title_subjectivity_abs_title_sentiment_polarity_interaction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2737b7bf-22eb-464c-9ddc-92f110091051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'M1': basic_text_features,\n",
    "    'M2': basic_text_features + content_properties,\n",
    "    'M3': basic_text_features + content_properties + keyword_performance,\n",
    "    'M4': basic_text_features + content_properties + keyword_performance + self_reference_metrics,\n",
    "    'M5': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing,\n",
    "    'M6': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment,\n",
    "    'M7': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment, \n",
    "    'M8': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment,\n",
    "    'M9': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + sqrd_basic_text_features,\n",
    "    'M10': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + sqrd_basic_text_features + interaction_basic_text_features,\n",
    "    'M11': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + sqrd_basic_text_features + interaction_basic_text_features + interaction_title_sentiment_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8862b8ee-e9e5-4001-933b-7c5ebf93dc91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 Validation Accuracy: 0.8734\n",
      "M2 Validation Accuracy: 0.8735\n",
      "M3 Validation Accuracy: 0.8732\n",
      "M4 Validation Accuracy: 0.8724\n",
      "M5 Validation Accuracy: 0.8724\n",
      "M6 Validation Accuracy: 0.8732\n",
      "M7 Validation Accuracy: 0.8730\n",
      "M8 Validation Accuracy: 0.8730\n",
      "M9 Validation Accuracy: 0.8730\n",
      "M10 Validation Accuracy: 0.8739\n",
      "M11 Validation Accuracy: 0.8737\n"
     ]
    }
   ],
   "source": [
    "# Split 'train_data' into training and validation sets\n",
    "X = train_data.drop(['is_popular', 'timedelta', 'article_id'], axis=1)\n",
    "y = train_data['is_popular']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=20240407)\n",
    "\n",
    "# Iterate through each model, fit, and evaluate on the validation set\n",
    "for model_name, features in models.items():\n",
    "    # Select only the features for the current model increment\n",
    "    X_train_subset = X_train[features]\n",
    "    X_val_subset = X_val[features]\n",
    "\n",
    "    # Create a pipeline with standard scaling and logistic regression\n",
    "    pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "    # Fit the model on the training subset\n",
    "    pipeline.fit(X_train_subset, y_train)\n",
    "\n",
    "    # Predict on the validation subset and evaluate\n",
    "    y_pred_val = pipeline.predict(X_val_subset)\n",
    "    accuracy = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "    print(f'{model_name} Validation Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e9d75-7692-49aa-94ad-ee2f2509baef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d2a2c-a674-41e6-bd84-e8e078e6320d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b30b2eb4-a0bc-48e0-b09d-15b9f31a4c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training AUC</th>\n",
       "      <th>Validation AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M1 Logistic Regression</td>\n",
       "      <td>0.548108</td>\n",
       "      <td>0.555135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M2 Logistic Regression</td>\n",
       "      <td>0.624687</td>\n",
       "      <td>0.627810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M3 Logistic Regression</td>\n",
       "      <td>0.682657</td>\n",
       "      <td>0.686424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M4 Logistic Regression</td>\n",
       "      <td>0.686342</td>\n",
       "      <td>0.688129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M5 Logistic Regression</td>\n",
       "      <td>0.687929</td>\n",
       "      <td>0.685065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M6 Logistic Regression</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.694652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M7 Logistic Regression</td>\n",
       "      <td>0.694248</td>\n",
       "      <td>0.695419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M8 Logistic Regression</td>\n",
       "      <td>0.695059</td>\n",
       "      <td>0.696528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M9 Logistic Regression</td>\n",
       "      <td>0.695749</td>\n",
       "      <td>0.694356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M10 Logistic Regression</td>\n",
       "      <td>0.699248</td>\n",
       "      <td>0.694025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M11 Logistic Regression</td>\n",
       "      <td>0.699312</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Training AUC  Validation AUC\n",
       "0    M1 Logistic Regression      0.548108        0.555135\n",
       "1    M2 Logistic Regression      0.624687        0.627810\n",
       "2    M3 Logistic Regression      0.682657        0.686424\n",
       "3    M4 Logistic Regression      0.686342        0.688129\n",
       "4    M5 Logistic Regression      0.687929        0.685065\n",
       "5    M6 Logistic Regression      0.693258        0.694652\n",
       "6    M7 Logistic Regression      0.694248        0.695419\n",
       "7    M8 Logistic Regression      0.695059        0.696528\n",
       "8    M9 Logistic Regression      0.695749        0.694356\n",
       "9   M10 Logistic Regression      0.699248        0.694025\n",
       "10  M11 Logistic Regression      0.699312        0.694300"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, features in models.items():\n",
    "    # Append \"Logistic Regression\" to the model name for clarity\n",
    "    full_model_name = f\"{model_name} Logistic Regression\"\n",
    "\n",
    "    # Define steps for pipeline: feature scaling and logistic regression\n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale\", StandardScaler(), features)], remainder='drop')),\n",
    "        (\"log_reg\", LogisticRegression())\n",
    "    ]\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Fit the model on training data\n",
    "    pipeline.fit(X_train[features], y_train)\n",
    "\n",
    "    # Predict probabilities on the training and validation data\n",
    "    # Note: We use predict_proba to get probabilities, and we're interested in the probabilities of the positive class (usually at index 1)\n",
    "    train_prob = pipeline.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = pipeline.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "    # Append results\n",
    "    results.append([full_model_name, train_auc, val_auc])\n",
    "\n",
    "# Adjust DataFrame to accommodate new results format\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Training AUC', 'Validation AUC'])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4cc9c-847e-4cf5-9489-42a1a836189a",
   "metadata": {},
   "source": [
    "## Lasso Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d8aff7a-c8f9-410f-9b69-231748b00629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed M1 in 0.34 seconds\n",
      "Completed M2 in 0.38 seconds\n",
      "Completed M3 in 0.50 seconds\n",
      "Completed M4 in 0.60 seconds\n",
      "Completed M5 in 0.60 seconds\n",
      "Completed M6 in 0.89 seconds\n",
      "Completed M7 in 0.91 seconds\n",
      "Completed M8 in 1.03 seconds\n",
      "Completed M9 in 1.50 seconds\n",
      "Completed M10 in 2.15 seconds\n",
      "Completed M11 in 2.20 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training AUC</th>\n",
       "      <th>Validation AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M1 Logistic Regression</td>\n",
       "      <td>0.548108</td>\n",
       "      <td>0.555135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M2 Logistic Regression</td>\n",
       "      <td>0.624687</td>\n",
       "      <td>0.627810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M3 Logistic Regression</td>\n",
       "      <td>0.682657</td>\n",
       "      <td>0.686424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M4 Logistic Regression</td>\n",
       "      <td>0.686342</td>\n",
       "      <td>0.688129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M5 Logistic Regression</td>\n",
       "      <td>0.687929</td>\n",
       "      <td>0.685065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M6 Logistic Regression</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.694652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M7 Logistic Regression</td>\n",
       "      <td>0.694248</td>\n",
       "      <td>0.695419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M8 Logistic Regression</td>\n",
       "      <td>0.695059</td>\n",
       "      <td>0.696528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M9 Logistic Regression</td>\n",
       "      <td>0.695749</td>\n",
       "      <td>0.694356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M10 Logistic Regression</td>\n",
       "      <td>0.699248</td>\n",
       "      <td>0.694025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M11 Logistic Regression</td>\n",
       "      <td>0.699312</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M1 Lasso</td>\n",
       "      <td>0.523024</td>\n",
       "      <td>0.531205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M2 Lasso</td>\n",
       "      <td>0.589911</td>\n",
       "      <td>0.592422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M3 Lasso</td>\n",
       "      <td>0.629210</td>\n",
       "      <td>0.625074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M4 Lasso</td>\n",
       "      <td>0.631790</td>\n",
       "      <td>0.629762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M5 Lasso</td>\n",
       "      <td>0.635164</td>\n",
       "      <td>0.632033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M6 Lasso</td>\n",
       "      <td>0.643302</td>\n",
       "      <td>0.647055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M7 Lasso</td>\n",
       "      <td>0.641714</td>\n",
       "      <td>0.653629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M8 Lasso</td>\n",
       "      <td>0.642905</td>\n",
       "      <td>0.649432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M9 Lasso</td>\n",
       "      <td>0.641714</td>\n",
       "      <td>0.646391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M10 Lasso</td>\n",
       "      <td>0.644096</td>\n",
       "      <td>0.643802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M11 Lasso</td>\n",
       "      <td>0.644691</td>\n",
       "      <td>0.646853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Training AUC  Validation AUC\n",
       "0    M1 Logistic Regression      0.548108        0.555135\n",
       "1    M2 Logistic Regression      0.624687        0.627810\n",
       "2    M3 Logistic Regression      0.682657        0.686424\n",
       "3    M4 Logistic Regression      0.686342        0.688129\n",
       "4    M5 Logistic Regression      0.687929        0.685065\n",
       "5    M6 Logistic Regression      0.693258        0.694652\n",
       "6    M7 Logistic Regression      0.694248        0.695419\n",
       "7    M8 Logistic Regression      0.695059        0.696528\n",
       "8    M9 Logistic Regression      0.695749        0.694356\n",
       "9   M10 Logistic Regression      0.699248        0.694025\n",
       "10  M11 Logistic Regression      0.699312        0.694300\n",
       "11                 M1 Lasso      0.523024        0.531205\n",
       "12                 M2 Lasso      0.589911        0.592422\n",
       "13                 M3 Lasso      0.629210        0.625074\n",
       "14                 M4 Lasso      0.631790        0.629762\n",
       "15                 M5 Lasso      0.635164        0.632033\n",
       "16                 M6 Lasso      0.643302        0.647055\n",
       "17                 M7 Lasso      0.641714        0.653629\n",
       "18                 M8 Lasso      0.642905        0.649432\n",
       "19                 M9 Lasso      0.641714        0.646391\n",
       "20                M10 Lasso      0.644096        0.643802\n",
       "21                M11 Lasso      0.644691        0.646853"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale_numeric_features\", MinMaxScaler(), features)], remainder='drop')),\n",
    "        (\"lasso\", LassoCV())\n",
    "    ]\n",
    "    pipe_lasso = Pipeline(steps)\n",
    "    pipe_lasso.fit(X_train[features], y_train)\n",
    "\n",
    "    # Assuming y_train and y_val are binary and you're treating Lasso predictions as continuous scores\n",
    "    train_scores = pipe_lasso.predict(X_train[features])\n",
    "    val_scores = pipe_lasso.predict(X_val[features])\n",
    "\n",
    "    # Convert scores to binary predictions based on a threshold, e.g., the median\n",
    "    threshold = np.median(train_scores)  # Simple threshold example\n",
    "    train_pred = np.where(train_scores > threshold, 1, 0)\n",
    "    val_pred = np.where(val_scores > threshold, 1, 0)\n",
    "\n",
    "    # Calculate AUC scores\n",
    "    train_auc = roc_auc_score(y_train, train_pred)\n",
    "    val_auc = roc_auc_score(y_val, val_pred)\n",
    "\n",
    "    new_row = pd.DataFrame([[f\"{group_name} Lasso\", train_auc, val_auc]], columns=['Model', 'Training AUC', 'Validation AUC'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3f4f8-56ac-483b-8dbf-0bb64ae96f02",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b239e1-c123-43da-be03-90ed65695d1c",
   "metadata": {},
   "source": [
    "## Gradient Boosted Random Forest\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a884036-8492-4ab0-b79d-01149db92e3e",
   "metadata": {},
   "source": [
    "## Explainable Boosting Machine\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40007e87-defa-4e48-9e55-a512a9d43122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# as EBM is based on trees, it can handle categorical features so no need for one-hot-encoding\n",
    "ebm_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")), # only imput missing values\n",
    "    (\"ebm\", ExplainableBoostingClassifier(random_state=20240325))  # cannot accept RandomState directly\n",
    "])\n",
    "ebm_pipe.fit(X_train, y_train)  \n",
    "\n",
    "auc = roc_auc_score(y_test, ebm_pipe.predict_proba(X_test)[:, 1])\n",
    "print(f\"AUC on the test set for EBM is {round(auc, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0b07d-bede-4715-9b44-63dad2582cd8",
   "metadata": {},
   "source": [
    "## Neural Network Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a33e3-90b3-4668-a5a5-4165f874974d",
   "metadata": {},
   "source": [
    "# Hypertuning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bca990-82f6-4e01-b72d-7bf383d0bd02",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4abceeb-03e3-47d2-a333-c3d5d3d2582e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
