{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c920e6-3761-4d94-9558-ae995ffa39ac",
   "metadata": {},
   "source": [
    "# <center> Kaggle Competition Assignment <center> Ian Brandenburg (2304791) <center> [GitHub Repo](https://github.com/Iandrewburg/Data_Science/tree/main/Data_Science_2/Assignments/Take_Home_Final)\n",
    "    \n",
    "    \n",
    "The Kaggle competition has been launched, please register using this [link](https://www.kaggle.com/t/f79b637ede074e70a233661b4614083c).\n",
    "\n",
    "You will find the training and test data in the data section of the competition, along with a description of the features. You will need to build models on the training data and make predictions on the test data and submit your solutions to Kaggle. You will also find a sample solution file in the data section that shows the format you will need to use for your own submissions.\n",
    "\n",
    "The deadline for Kaggle solutions is 8PM on 19 April. You will be graded primarily on the basis of your work and how clearly you explain your methods and results. Those in the top three in the competition will receive some extra points. I expect you to experiment with all the methods we have covered: linear models, random forest, gradient boosting, neural networks + parameter tuning, feature engineering.\n",
    "\n",
    "You will see the public score of your best model on the leaderboard. A private dataset will be used to evaluate the final performance of your model to avoid overfitting based on the leaderboard.\n",
    "\n",
    "You should also submit to Moodle the documentation (ipynb and pdf) of your work, including exploratory data analysis, data cleaning, parameter tuning and evaluation. Aim for concise explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156211d-6705-4669-9a65-835e9a896f55",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f6fb47-2ef1-43d6-94ec-a6482b8525dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "# Sklearn model selection, preprocessing, metrics, and ensemble methods\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Sklearn pipeline utilities\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Cat Boost Classifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Light GBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "# InterpretML for explainable boosting\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# TensorFlow and Keras for neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3e2f2-32fb-405c-909f-ed9b42a5b7fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Wrangling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044cd425-2b1e-4019-91d4-976186fb8854",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30844dd-201d-416c-80dc-e0040f99a95a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>is_popular</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>594</td>\n",
       "      <td>9</td>\n",
       "      <td>702</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.620438</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.153395</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346</td>\n",
       "      <td>8</td>\n",
       "      <td>1197</td>\n",
       "      <td>0.470143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666209</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.308167</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>484</td>\n",
       "      <td>9</td>\n",
       "      <td>214</td>\n",
       "      <td>0.618090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>639</td>\n",
       "      <td>8</td>\n",
       "      <td>249</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.664740</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177</td>\n",
       "      <td>12</td>\n",
       "      <td>1219</td>\n",
       "      <td>0.397841</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.583578</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.441111</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0        594               9               702         0.454545   \n",
       "1        346               8              1197         0.470143   \n",
       "2        484               9               214         0.618090   \n",
       "3        639               8               249         0.621951   \n",
       "4        177              12              1219         0.397841   \n",
       "\n",
       "   n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0               1.0                  0.620438         11               2   \n",
       "1               1.0                  0.666209         21               6   \n",
       "2               1.0                  0.748092          5               2   \n",
       "3               1.0                  0.664740         16               5   \n",
       "4               1.0                  0.583578         21               1   \n",
       "\n",
       "   num_imgs  num_videos  ...  max_positive_polarity  avg_negative_polarity  \\\n",
       "0         1           0  ...               1.000000              -0.153395   \n",
       "1         2          13  ...               1.000000              -0.308167   \n",
       "2         1           0  ...               0.433333              -0.141667   \n",
       "3         8           0  ...               0.500000              -0.500000   \n",
       "4         1           2  ...               0.800000              -0.441111   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                   -0.4                  -0.10                 0.0   \n",
       "1                   -1.0                  -0.10                 0.0   \n",
       "2                   -0.2                  -0.05                 0.0   \n",
       "3                   -0.8                  -0.40                 0.0   \n",
       "4                   -1.0                  -0.05                 0.0   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                       0.0                     0.5   \n",
       "1                       0.0                     0.5   \n",
       "2                       0.0                     0.5   \n",
       "3                       0.0                     0.5   \n",
       "4                       0.0                     0.5   \n",
       "\n",
       "   abs_title_sentiment_polarity  is_popular  article_id  \n",
       "0                           0.0           0           1  \n",
       "1                           0.0           0           3  \n",
       "2                           0.0           0           5  \n",
       "3                           0.0           0           6  \n",
       "4                           0.0           0           7  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"https://raw.githubusercontent.com/Iandrewburg/Data_Science/main/Data_Science_2/Assignments/Take_Home_Final/train.csv\")\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bbc1b2-429b-465f-9e29-d83e646b31cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134</td>\n",
       "      <td>11</td>\n",
       "      <td>217</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.818966</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.170370</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>-0.155556</td>\n",
       "      <td>0.211111</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>415</td>\n",
       "      <td>11</td>\n",
       "      <td>1041</td>\n",
       "      <td>0.489423</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700321</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.426268</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>625</td>\n",
       "      <td>9</td>\n",
       "      <td>486</td>\n",
       "      <td>0.599585</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.387821</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148</td>\n",
       "      <td>14</td>\n",
       "      <td>505</td>\n",
       "      <td>0.509018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718861</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.284722</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>294</td>\n",
       "      <td>14</td>\n",
       "      <td>274</td>\n",
       "      <td>0.620301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0        134              11               217         0.631579   \n",
       "1        415              11              1041         0.489423   \n",
       "2        625               9               486         0.599585   \n",
       "3        148              14               505         0.509018   \n",
       "4        294              14               274         0.620301   \n",
       "\n",
       "   n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0               1.0                  0.818966          4               2   \n",
       "1               1.0                  0.700321         22               3   \n",
       "2               1.0                  0.727273          4               3   \n",
       "3               1.0                  0.718861          8               4   \n",
       "4               1.0                  0.726190          5               1   \n",
       "\n",
       "   num_imgs  num_videos  ...  min_positive_polarity  max_positive_polarity  \\\n",
       "0         2           0  ...               0.136364                    0.5   \n",
       "1         0          14  ...               0.050000                    1.0   \n",
       "2         1           0  ...               0.062500                    0.7   \n",
       "3         1           1  ...               0.100000                    1.0   \n",
       "4         1           0  ...               0.100000                    0.6   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.170370              -0.200000              -0.155556   \n",
       "1              -0.426268              -1.000000              -0.100000   \n",
       "2              -0.387821              -1.000000              -0.050000   \n",
       "3              -0.284722              -0.400000              -0.050000   \n",
       "4              -0.333333              -0.333333              -0.333333   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.288889                 -0.155556                0.211111   \n",
       "1            0.975000                  0.300000                0.475000   \n",
       "2            0.000000                  0.000000                0.500000   \n",
       "3            0.000000                  0.000000                0.500000   \n",
       "4            0.000000                  0.000000                0.500000   \n",
       "\n",
       "   abs_title_sentiment_polarity  article_id  \n",
       "0                      0.155556           2  \n",
       "1                      0.300000           4  \n",
       "2                      0.000000          10  \n",
       "3                      0.000000          13  \n",
       "4                      0.000000          26  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"https://raw.githubusercontent.com/Iandrewburg/Data_Science/main/Data_Science_2/Assignments/Take_Home_Final/test.csv\")\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdaeb309-5690-43a6-a411-a7a56c7aa3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
       "       'num_keywords', 'data_channel_is_lifestyle',\n",
       "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
       "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
       "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
       "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
       "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
       "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
       "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
       "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
       "       'max_positive_polarity', 'avg_negative_polarity',\n",
       "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'article_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced915d4-89b8-49e9-a4a2-84e12f0aa288",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854cf70-2030-4ae0-bc31-b84b3c68f513",
   "metadata": {},
   "source": [
    "### Variable Descriptions\n",
    "---\n",
    "\n",
    "\n",
    "    timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
    "    n_tokens_title: Number of words in the title\n",
    "    n_tokens_content: Number of words in the content\n",
    "    n_unique_tokens: Rate of unique words in the content\n",
    "    n_non_stop_words: Rate of non-stop words in the content\n",
    "    n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
    "    num_hrefs: Number of links\n",
    "    num_self_hrefs: Number of links to other articles published by Mashable\n",
    "    num_imgs: Number of images\n",
    "    num_videos: Number of videos\n",
    "    average_token_length: Average length of the words in the content\n",
    "    num_keywords: Number of keywords in the metadata\n",
    "    data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
    "    data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
    "    data_channel_is_bus: Is data channel 'Business'?\n",
    "    data_channel_is_socmed: Is data channel 'Social Media'?\n",
    "    data_channel_is_tech: Is data channel 'Tech'?\n",
    "    data_channel_is_world: Is data channel 'World'?\n",
    "    kw_min_min: Worst keyword (min. shares)\n",
    "    kw_max_min: Worst keyword (max. shares)\n",
    "    kw_avg_min: Worst keyword (avg. shares)\n",
    "    kw_min_max: Best keyword (min. shares)\n",
    "    kw_max_max: Best keyword (max. shares)\n",
    "    kw_avg_max: Best keyword (avg. shares)\n",
    "    kw_min_avg: Avg. keyword (min. shares)\n",
    "    kw_max_avg: Avg. keyword (max. shares)\n",
    "    kw_avg_avg: Avg. keyword (avg. shares)\n",
    "    self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
    "    self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
    "    self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
    "    weekday_is_monday: Was the article published on a Monday?\n",
    "    weekday_is_tuesday: Was the article published on a Tuesday?\n",
    "    weekday_is_wednesday: Was the article published on a Wednesday?\n",
    "    weekday_is_thursday: Was the article published on a Thursday?\n",
    "    weekday_is_friday: Was the article published on a Friday?\n",
    "    weekday_is_saturday: Was the article published on a Saturday?\n",
    "    weekday_is_sunday: Was the article published on a Sunday?\n",
    "    is_weekend: Was the article published on the weekend?\n",
    "    LDA_00: Closeness to LDA topic 0\n",
    "    LDA_01: Closeness to LDA topic 1\n",
    "    LDA_02: Closeness to LDA topic 2\n",
    "    LDA_03: Closeness to LDA topic 3\n",
    "    LDA_04: Closeness to LDA topic 4\n",
    "    global_subjectivity: Text subjectivity\n",
    "    global_sentiment_polarity: Text sentiment polarity\n",
    "    global_rate_positive_words: Rate of positive words in the content\n",
    "    global_rate_negative_words: Rate of negative words in the content\n",
    "    rate_positive_words: Rate of positive words among non-neutral tokens\n",
    "    rate_negative_words: Rate of negative words among non-neutral tokens\n",
    "    avg_positive_polarity: Avg. polarity of positive words\n",
    "    min_positive_polarity: Min. polarity of positive words\n",
    "    max_positive_polarity: Max. polarity of positive words\n",
    "    avg_negative_polarity: Avg. polarity of negative words\n",
    "    min_negative_polarity: Min. polarity of negative words\n",
    "    max_negative_polarity: Max. polarity of negative words\n",
    "    title_subjectivity: Title subjectivity\n",
    "    title_sentiment_polarity: Title polarity\n",
    "    abs_title_subjectivity: Absolute subjectivity level\n",
    "    abs_title_sentiment_polarity: Absolute polarity level\n",
    "    is_popular: Whether or not the article was among the most popular ones based on shares on social media\n",
    "    article_id: Unique identifier of the article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942c303a-6a91-42f4-bfa2-4e1508a1a038",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>is_popular</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "      <td>29733.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>355.645646</td>\n",
       "      <td>10.390812</td>\n",
       "      <td>545.008274</td>\n",
       "      <td>0.555076</td>\n",
       "      <td>1.005852</td>\n",
       "      <td>0.695432</td>\n",
       "      <td>10.912690</td>\n",
       "      <td>3.290788</td>\n",
       "      <td>4.524535</td>\n",
       "      <td>1.263546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757780</td>\n",
       "      <td>-0.259709</td>\n",
       "      <td>-0.520981</td>\n",
       "      <td>-0.107793</td>\n",
       "      <td>0.281878</td>\n",
       "      <td>0.069691</td>\n",
       "      <td>0.341427</td>\n",
       "      <td>0.155234</td>\n",
       "      <td>0.121649</td>\n",
       "      <td>19834.913530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>214.288261</td>\n",
       "      <td>2.110135</td>\n",
       "      <td>469.358037</td>\n",
       "      <td>4.064572</td>\n",
       "      <td>6.039655</td>\n",
       "      <td>3.768796</td>\n",
       "      <td>11.316508</td>\n",
       "      <td>3.840874</td>\n",
       "      <td>8.213823</td>\n",
       "      <td>4.189080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247293</td>\n",
       "      <td>0.128488</td>\n",
       "      <td>0.290454</td>\n",
       "      <td>0.095672</td>\n",
       "      <td>0.323461</td>\n",
       "      <td>0.264379</td>\n",
       "      <td>0.188735</td>\n",
       "      <td>0.225066</td>\n",
       "      <td>0.326886</td>\n",
       "      <td>11432.376037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.471400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.626126</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328704</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9965.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>342.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>0.539894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690566</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.252827</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.144444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19859.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>545.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.755208</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186494</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29742.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>1042.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>39643.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "count  29733.000000    29733.000000      29733.000000     29733.000000   \n",
       "mean     355.645646       10.390812        545.008274         0.555076   \n",
       "std      214.288261        2.110135        469.358037         4.064572   \n",
       "min        8.000000        2.000000          0.000000         0.000000   \n",
       "25%      164.000000        9.000000        246.000000         0.471400   \n",
       "50%      342.000000       10.000000        409.000000         0.539894   \n",
       "75%      545.000000       12.000000        712.000000         0.609375   \n",
       "max      731.000000       23.000000       8474.000000       701.000000   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens     num_hrefs  \\\n",
       "count      29733.000000              29733.000000  29733.000000   \n",
       "mean           1.005852                  0.695432     10.912690   \n",
       "std            6.039655                  3.768796     11.316508   \n",
       "min            0.000000                  0.000000      0.000000   \n",
       "25%            1.000000                  0.626126      4.000000   \n",
       "50%            1.000000                  0.690566      8.000000   \n",
       "75%            1.000000                  0.755208     14.000000   \n",
       "max         1042.000000                650.000000    304.000000   \n",
       "\n",
       "       num_self_hrefs      num_imgs    num_videos  ...  max_positive_polarity  \\\n",
       "count    29733.000000  29733.000000  29733.000000  ...           29733.000000   \n",
       "mean         3.290788      4.524535      1.263546  ...               0.757780   \n",
       "std          3.840874      8.213823      4.189080  ...               0.247293   \n",
       "min          0.000000      0.000000      0.000000  ...               0.000000   \n",
       "25%          1.000000      1.000000      0.000000  ...               0.600000   \n",
       "50%          2.000000      1.000000      0.000000  ...               0.800000   \n",
       "75%          4.000000      4.000000      1.000000  ...               1.000000   \n",
       "max         74.000000    111.000000     91.000000  ...               1.000000   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "count           29733.000000           29733.000000           29733.000000   \n",
       "mean               -0.259709              -0.520981              -0.107793   \n",
       "std                 0.128488               0.290454               0.095672   \n",
       "min                -1.000000              -1.000000              -1.000000   \n",
       "25%                -0.328704              -0.700000              -0.125000   \n",
       "50%                -0.252827              -0.500000              -0.100000   \n",
       "75%                -0.186494              -0.300000              -0.050000   \n",
       "max                 0.000000               0.000000               0.000000   \n",
       "\n",
       "       title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "count        29733.000000              29733.000000            29733.000000   \n",
       "mean             0.281878                  0.069691                0.341427   \n",
       "std              0.323461                  0.264379                0.188735   \n",
       "min              0.000000                 -1.000000                0.000000   \n",
       "25%              0.000000                  0.000000                0.166667   \n",
       "50%              0.144444                  0.000000                0.500000   \n",
       "75%              0.500000                  0.136364                0.500000   \n",
       "max              1.000000                  1.000000                0.500000   \n",
       "\n",
       "       abs_title_sentiment_polarity    is_popular    article_id  \n",
       "count                  29733.000000  29733.000000  29733.000000  \n",
       "mean                       0.155234      0.121649  19834.913530  \n",
       "std                        0.225066      0.326886  11432.376037  \n",
       "min                        0.000000      0.000000      1.000000  \n",
       "25%                        0.000000      0.000000   9965.000000  \n",
       "50%                        0.000000      0.000000  19859.000000  \n",
       "75%                        0.250000      0.000000  29742.000000  \n",
       "max                        1.000000      1.000000  39643.000000  \n",
       "\n",
       "[8 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07d4d81-512a-4e49-9274-457839f87f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is 29733 rows, and 61 columns.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the training set is {train_data.shape[0]} rows, and {train_data.shape[1]} columns.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e62837-bcf9-4f77-9f5d-cc532454a9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 0 missing values in the dataset.\n"
     ]
    }
   ],
   "source": [
    "total_missing_values = train_data.isnull().sum()[train_data.isnull().sum() > 0].sum()\n",
    "print(f\"There are a total of {total_missing_values} missing values in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a44f9aa-7c8e-4556-831c-3542bffe08a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
       "       'num_keywords', 'data_channel_is_lifestyle',\n",
       "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
       "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
       "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
       "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
       "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
       "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
       "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
       "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
       "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
       "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
       "       'global_rate_negative_words', 'rate_positive_words',\n",
       "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
       "       'max_positive_polarity', 'avg_negative_polarity',\n",
       "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
       "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
       "       'abs_title_sentiment_polarity', 'is_popular', 'article_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fade411-c1c4-4598-8e61-0c8d2ea9a087",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54056a3b-786d-45e9-bbc5-df243acf9c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining variable groups\n",
    "basic_text_features = ['n_tokens_title',\n",
    "                       'n_tokens_content',\n",
    "                       'n_unique_tokens',\n",
    "                       'n_non_stop_words',\n",
    "                       'n_non_stop_unique_tokens',\n",
    "                       'average_token_length',\n",
    "                       'num_keywords']\n",
    "content_properties = ['num_hrefs',\n",
    "                      'num_self_hrefs',\n",
    "                      'num_imgs',\n",
    "                      'num_videos',\n",
    "                      'global_subjectivity',\n",
    "                      'global_sentiment_polarity',\n",
    "                      'global_rate_positive_words',\n",
    "                      'global_rate_negative_words']\n",
    "keyword_performance = ['kw_min_min',\n",
    "                       'kw_max_min',\n",
    "                       'kw_avg_min',\n",
    "                       'kw_min_max',\n",
    "                       'kw_max_max',\n",
    "                       'kw_avg_max',\n",
    "                       'kw_min_avg',\n",
    "                       'kw_max_avg',\n",
    "                       'kw_avg_avg']\n",
    "self_reference_metrics = ['self_reference_min_shares',\n",
    "                          'self_reference_max_shares',\n",
    "                          'self_reference_avg_sharess']\n",
    "\n",
    "# dropped 'weekday_is_monday' and 'is_weekend'\n",
    "publication_timing = ['weekday_is_tuesday',\n",
    "                      'weekday_is_wednesday',\n",
    "                      'weekday_is_thursday',\n",
    "                      'weekday_is_friday',\n",
    "                      'weekday_is_saturday',\n",
    "                      'weekday_is_sunday']\n",
    "\n",
    "# dropped 'data_channel_is_lifestyle'\n",
    "content_topic_and_sentiment = ['data_channel_is_entertainment',\n",
    "                               'data_channel_is_bus',\n",
    "                               'data_channel_is_socmed',\n",
    "                               'data_channel_is_tech',\n",
    "                               'data_channel_is_world',\n",
    "                               'LDA_00',\n",
    "                               'LDA_01',\n",
    "                               'LDA_02',\n",
    "                               'LDA_03',\n",
    "                               'LDA_04',\n",
    "                               'rate_positive_words',\n",
    "                               'rate_negative_words',\n",
    "                               'avg_positive_polarity',\n",
    "                               'min_positive_polarity', \n",
    "                               'max_positive_polarity',\n",
    "                               'avg_negative_polarity',\n",
    "                               'min_negative_polarity',\n",
    "                               'max_negative_polarity']\n",
    "title_sentiment = ['title_subjectivity',\n",
    "                   'title_sentiment_polarity',\n",
    "                   'abs_title_subjectivity',\n",
    "                   'abs_title_sentiment_polarity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9de3fc-5a7d-4191-824d-820b00ad7120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def square_features(variables, df): \n",
    "    sqaured_features = []\n",
    "    for var in variables:\n",
    "        feature_name = f'{var}_squared'\n",
    "        df[feature_name] = df[var] ** 2\n",
    "        sqaured_features.append(feature_name)\n",
    "    return sqaured_features\n",
    "\n",
    "def cube_features(variables, df): \n",
    "    cubed_features = []\n",
    "    for var in variables:\n",
    "        feature_name = f'{var}_cubed'\n",
    "        df[feature_name] = df[var] ** 3\n",
    "        cubed_features.append(feature_name)\n",
    "    return cubed_features\n",
    "        \n",
    "def interact_features(variables, df):\n",
    "    interacted_features = []\n",
    "    for (var1, var2) in combinations(variables, 2):\n",
    "        feature_name = f'{var1}_{var2}_interaction'\n",
    "        df[feature_name] = df[var1] * df[var2]\n",
    "        interacted_features.append(feature_name)\n",
    "    return interacted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf2eb71-32a4-4a62-8c66-9331de8e3cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['self_reference_min_shares_self_reference_max_shares_interaction',\n",
       " 'self_reference_min_shares_self_reference_avg_sharess_interaction',\n",
       " 'self_reference_max_shares_self_reference_avg_sharess_interaction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################SQUARED TERMS###################\n",
    "# square basic features\n",
    "sqrd_basic_text_features = square_features(basic_text_features, train_data)\n",
    "square_features(basic_text_features, test_data)\n",
    "    \n",
    "# square title sentiment features\n",
    "sqrd_title_sentiment = square_features(title_sentiment, train_data)\n",
    "square_features(title_sentiment, test_data)\n",
    "\n",
    "# square content properties\n",
    "sqrd_content_properties = square_features(content_properties, train_data)\n",
    "square_features(content_properties, test_data)\n",
    "\n",
    "# square keyword performance\n",
    "sqrd_keyword_performance = square_features(keyword_performance, train_data)\n",
    "square_features(keyword_performance, test_data)\n",
    "\n",
    "# square self reference metrics\n",
    "sqrd_self_reference_metrics = square_features(self_reference_metrics, train_data)\n",
    "square_features(self_reference_metrics, test_data)\n",
    "\n",
    "##################CUBED TERMS###################\n",
    "# CUBED basic features\n",
    "cube_basic_text_features = cube_features(basic_text_features, train_data)\n",
    "cube_features(basic_text_features, test_data)\n",
    "    \n",
    "# CUBED title sentiment features\n",
    "cube_title_sentiment = cube_features(title_sentiment, train_data)\n",
    "cube_features(title_sentiment, test_data)\n",
    "\n",
    "# CUBED content properties\n",
    "cube_content_properties = cube_features(content_properties, train_data)\n",
    "cube_features(content_properties, test_data)\n",
    "\n",
    "# CUBED keyword performance\n",
    "cube_keyword_performance = cube_features(keyword_performance, train_data)\n",
    "cube_features(keyword_performance, test_data)\n",
    "\n",
    "# CUBED self reference metrics\n",
    "cube_self_reference_metrics = cube_features(self_reference_metrics, train_data)\n",
    "cube_features(self_reference_metrics, test_data)\n",
    "  \n",
    "################INTERACTION TERMS##################\n",
    "# Interacting the basic features\n",
    "interaction_basic_text_features = interact_features(basic_text_features, train_data)\n",
    "interact_features(basic_text_features, test_data)\n",
    "\n",
    "# Interacting the title sentiment features\n",
    "interaction_title_sentiment = interact_features(title_sentiment, train_data)\n",
    "interact_features(title_sentiment, test_data)\n",
    "\n",
    "# Interacting content properties\n",
    "interaction_content_properties = interact_features(content_properties, train_data)\n",
    "interact_features(content_properties, test_data)\n",
    "\n",
    "# Interacting keyword performance\n",
    "interaction_keyword_performance = interact_features(keyword_performance, train_data)\n",
    "interact_features(keyword_performance, test_data)\n",
    "\n",
    "# Interacting self reference metrics\n",
    "interaction_self_reference_metrics = interact_features(self_reference_metrics, train_data)\n",
    "interact_features(self_reference_metrics, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e44e2b7-5e58-49df-91b8-a3d33b6601bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perm_importance_variables = ['n_tokens_title',\n",
    "                             'n_tokens_content',\n",
    "                             'n_unique_tokens',\n",
    "                             'n_non_stop_words',\n",
    "                             'n_non_stop_unique_tokens',\n",
    "                             'average_token_length',\n",
    "                             'num_keywords',\n",
    "                             'num_hrefs', \n",
    "                             'num_self_hrefs', \n",
    "                             'num_imgs', \n",
    "                             'num_videos', \n",
    "                             'global_subjectivity', \n",
    "                             'global_sentiment_polarity', \n",
    "                             'kw_min_min', \n",
    "                             'kw_max_min', \n",
    "                             'kw_avg_min', \n",
    "                             'kw_min_max', \n",
    "                             'kw_max_max',\n",
    "                             'kw_avg_max', \n",
    "                             'kw_min_avg', \n",
    "                             'kw_max_avg', \n",
    "                             'kw_avg_avg', \n",
    "                             'self_reference_min_shares', \n",
    "                             'self_reference_max_shares', \n",
    "                             'self_reference_avg_sharess',\n",
    "                             'weekday_is_thursday', \n",
    "                             'weekday_is_friday',\n",
    "                             'weekday_is_sunday', \n",
    "                             'data_channel_is_entertainment',\n",
    "                             'data_channel_is_bus', \n",
    "                             'data_channel_is_socmed', \n",
    "                             'data_channel_is_tech', \n",
    "                             'data_channel_is_world', \n",
    "                             'LDA_00', \n",
    "                             'LDA_01',\n",
    "                             'LDA_02', \n",
    "                             'LDA_03', \n",
    "                             'LDA_04', \n",
    "                             'rate_positive_words', \n",
    "                             'avg_positive_polarity',\n",
    "                             'min_positive_polarity', \n",
    "                             'avg_negative_polarity', \n",
    "                             'min_negative_polarity', \n",
    "                             'max_negative_polarity', \n",
    "                             'title_subjectivity',\n",
    "                             'abs_title_subjectivity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be410bba-7dd2-4be0-934a-2af6b6526caf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timedelta', 'n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
       "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
       "       'num_self_hrefs', 'num_imgs', 'num_videos',\n",
       "       ...\n",
       "       'kw_max_max_kw_avg_avg_interaction',\n",
       "       'kw_avg_max_kw_min_avg_interaction',\n",
       "       'kw_avg_max_kw_max_avg_interaction',\n",
       "       'kw_avg_max_kw_avg_avg_interaction',\n",
       "       'kw_min_avg_kw_max_avg_interaction',\n",
       "       'kw_min_avg_kw_avg_avg_interaction',\n",
       "       'kw_max_avg_kw_avg_avg_interaction',\n",
       "       'self_reference_min_shares_self_reference_max_shares_interaction',\n",
       "       'self_reference_min_shares_self_reference_avg_sharess_interaction',\n",
       "       'self_reference_max_shares_self_reference_avg_sharess_interaction'],\n",
       "      dtype='object', length=216)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2737b7bf-22eb-464c-9ddc-92f110091051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'M1': basic_text_features,\n",
    "    'M2': basic_text_features + content_properties,\n",
    "    'M3': basic_text_features + content_properties + keyword_performance,\n",
    "    'M4': basic_text_features + content_properties + keyword_performance + self_reference_metrics,\n",
    "    'M5': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing,\n",
    "    'M6': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment,\n",
    "    'M7': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment, \n",
    "    'M8': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment,\n",
    "    'M9': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + sqrd_basic_text_features,\n",
    "    'M10': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + sqrd_basic_text_features + interaction_basic_text_features,\n",
    "    'M11': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + sqrd_basic_text_features + interaction_basic_text_features + interaction_title_sentiment,\n",
    "    'M12': perm_importance_variables,\n",
    "    'M13': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_content_properties + sqrd_keyword_performance,\n",
    "    'M14': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + sqrd_basic_text_features + sqrd_content_properties,\n",
    "    'M15': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + sqrd_basic_text_features + sqrd_content_properties + sqrd_keyword_performance,\n",
    "    'M16': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + sqrd_basic_text_features + sqrd_content_properties + sqrd_keyword_performance + sqrd_self_reference_metrics,\n",
    "    'M17': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + interaction_content_properties,\n",
    "    'M18': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + interaction_content_properties + interaction_keyword_performance,\n",
    "    'M19': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + interaction_content_properties + interaction_keyword_performance + interaction_self_reference_metrics,\n",
    "    'M20': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + interaction_content_properties + interaction_keyword_performance + interaction_self_reference_metrics + sqrd_title_sentiment + sqrd_basic_text_features + sqrd_content_properties + sqrd_keyword_performance + sqrd_self_reference_metrics,\n",
    "    'M21': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + interaction_content_properties + interaction_basic_text_features + interaction_title_sentiment + interaction_keyword_performance + interaction_self_reference_metrics + sqrd_title_sentiment + sqrd_basic_text_features + sqrd_content_properties + sqrd_keyword_performance + sqrd_self_reference_metrics,\n",
    "    'M22': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment, \n",
    "    'M23': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment + sqrd_basic_text_features + cube_basic_text_features, \n",
    "    'M24': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment + sqrd_basic_text_features + cube_basic_text_features + sqrd_content_properties + cube_content_properties, \n",
    "    'M25': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment + sqrd_basic_text_features + cube_basic_text_features + sqrd_content_properties + cube_content_properties + sqrd_keyword_performance + cube_keyword_performance, \n",
    "    'M26': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment + sqrd_basic_text_features + cube_basic_text_features + sqrd_content_properties + cube_content_properties + sqrd_keyword_performance + cube_keyword_performance + sqrd_self_reference_metrics + cube_self_reference_metrics, \n",
    "    'M27': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment + sqrd_basic_text_features + cube_basic_text_features + sqrd_content_properties + cube_content_properties + sqrd_keyword_performance + cube_keyword_performance + sqrd_self_reference_metrics + cube_self_reference_metrics + interaction_content_properties, \n",
    "    'M28': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment + sqrd_basic_text_features + cube_basic_text_features + sqrd_content_properties + cube_content_properties + sqrd_keyword_performance + cube_keyword_performance + sqrd_self_reference_metrics + cube_self_reference_metrics + interaction_content_properties + interaction_basic_text_features, \n",
    "    'M29': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment + sqrd_basic_text_features + cube_basic_text_features + sqrd_content_properties + cube_content_properties + sqrd_keyword_performance + cube_keyword_performance + sqrd_self_reference_metrics + cube_self_reference_metrics + interaction_content_properties + interaction_basic_text_features + interaction_title_sentiment, \n",
    "    'M30': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment + sqrd_basic_text_features + cube_basic_text_features + sqrd_content_properties + cube_content_properties + sqrd_keyword_performance + cube_keyword_performance + sqrd_self_reference_metrics + cube_self_reference_metrics + interaction_content_properties + interaction_basic_text_features + interaction_title_sentiment + interaction_keyword_performance, \n",
    "    'M31': basic_text_features + content_properties + keyword_performance + self_reference_metrics + publication_timing + content_topic_and_sentiment + title_sentiment + sqrd_title_sentiment + cube_title_sentiment + sqrd_basic_text_features + cube_basic_text_features + sqrd_content_properties + cube_content_properties + sqrd_keyword_performance + cube_keyword_performance + sqrd_self_reference_metrics + cube_self_reference_metrics + interaction_content_properties + interaction_basic_text_features + interaction_title_sentiment + interaction_keyword_performance + interaction_self_reference_metrics \n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8862b8ee-e9e5-4001-933b-7c5ebf93dc91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split 'train_data' into training and validation sets\n",
    "X = train_data.drop(['is_popular', 'timedelta', 'article_id'], axis=1)\n",
    "y = train_data['is_popular']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=20240407)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e9d75-7692-49aa-94ad-ee2f2509baef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f67c4ba-37f8-4050-9d98-2e5b1340589d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculateRMSLE(prediction, y_obs):\n",
    "    return round(np.sqrt(\n",
    "        np.mean(\n",
    "            (\n",
    "                np.log(np.where(prediction < 0, 0, prediction) + 1) - \n",
    "                np.log(y_obs + 1)\n",
    "            )**2\n",
    "        )\n",
    "    ), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c5dfe9e-3f83-446c-bc61-0b76563f6bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initilialize results list\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d2a2c-a674-41e6-bd84-e8e078e6320d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2103b-4079-4f67-86b3-3d4f82da990d",
   "metadata": {},
   "source": [
    "### Simple Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b30b2eb4-a0bc-48e0-b09d-15b9f31a4c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training AUC</th>\n",
       "      <th>Validation AUC</th>\n",
       "      <th>Training RMSLE</th>\n",
       "      <th>Validation RMSLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M1 Logistic Regression</td>\n",
       "      <td>0.548108</td>\n",
       "      <td>0.555135</td>\n",
       "      <td>0.2271</td>\n",
       "      <td>0.2314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M2 Logistic Regression</td>\n",
       "      <td>0.624687</td>\n",
       "      <td>0.627810</td>\n",
       "      <td>0.2253</td>\n",
       "      <td>0.2291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M3 Logistic Regression</td>\n",
       "      <td>0.682657</td>\n",
       "      <td>0.686424</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>0.2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M4 Logistic Regression</td>\n",
       "      <td>0.686342</td>\n",
       "      <td>0.688129</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>0.2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M5 Logistic Regression</td>\n",
       "      <td>0.687915</td>\n",
       "      <td>0.684988</td>\n",
       "      <td>0.2223</td>\n",
       "      <td>0.2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M6 Logistic Regression</td>\n",
       "      <td>0.693311</td>\n",
       "      <td>0.694309</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.2255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M7 Logistic Regression</td>\n",
       "      <td>0.694318</td>\n",
       "      <td>0.695099</td>\n",
       "      <td>0.2219</td>\n",
       "      <td>0.2253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M8 Logistic Regression</td>\n",
       "      <td>0.695176</td>\n",
       "      <td>0.696331</td>\n",
       "      <td>0.2219</td>\n",
       "      <td>0.2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M9 Logistic Regression</td>\n",
       "      <td>0.695775</td>\n",
       "      <td>0.694353</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M10 Logistic Regression</td>\n",
       "      <td>0.699305</td>\n",
       "      <td>0.693886</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M11 Logistic Regression</td>\n",
       "      <td>0.699465</td>\n",
       "      <td>0.694220</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M12 Logistic Regression</td>\n",
       "      <td>0.692398</td>\n",
       "      <td>0.697022</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M13 Logistic Regression</td>\n",
       "      <td>0.701580</td>\n",
       "      <td>0.700482</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>0.2245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M14 Logistic Regression</td>\n",
       "      <td>0.695972</td>\n",
       "      <td>0.694173</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M15 Logistic Regression</td>\n",
       "      <td>0.702613</td>\n",
       "      <td>0.699578</td>\n",
       "      <td>0.2212</td>\n",
       "      <td>0.2244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>M16 Logistic Regression</td>\n",
       "      <td>0.709128</td>\n",
       "      <td>0.701346</td>\n",
       "      <td>0.2206</td>\n",
       "      <td>0.2247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>M17 Logistic Regression</td>\n",
       "      <td>0.697060</td>\n",
       "      <td>0.688289</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>M18 Logistic Regression</td>\n",
       "      <td>0.703661</td>\n",
       "      <td>0.687535</td>\n",
       "      <td>0.2211</td>\n",
       "      <td>0.2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M19 Logistic Regression</td>\n",
       "      <td>0.710787</td>\n",
       "      <td>0.688838</td>\n",
       "      <td>0.2205</td>\n",
       "      <td>0.2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M20 Logistic Regression</td>\n",
       "      <td>0.716079</td>\n",
       "      <td>0.695409</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>0.2258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M21 Logistic Regression</td>\n",
       "      <td>0.718754</td>\n",
       "      <td>0.696427</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.2255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M22 Logistic Regression</td>\n",
       "      <td>0.695714</td>\n",
       "      <td>0.696995</td>\n",
       "      <td>0.2219</td>\n",
       "      <td>0.2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>M23 Logistic Regression</td>\n",
       "      <td>0.697076</td>\n",
       "      <td>0.695949</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>M24 Logistic Regression</td>\n",
       "      <td>0.698787</td>\n",
       "      <td>0.695604</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>M25 Logistic Regression</td>\n",
       "      <td>0.708691</td>\n",
       "      <td>0.705051</td>\n",
       "      <td>0.2208</td>\n",
       "      <td>0.2240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>M26 Logistic Regression</td>\n",
       "      <td>0.715142</td>\n",
       "      <td>0.706303</td>\n",
       "      <td>0.2201</td>\n",
       "      <td>0.2244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>M27 Logistic Regression</td>\n",
       "      <td>0.716938</td>\n",
       "      <td>0.698923</td>\n",
       "      <td>0.2199</td>\n",
       "      <td>0.2253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>M28 Logistic Regression</td>\n",
       "      <td>0.719412</td>\n",
       "      <td>0.698379</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.2252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>M29 Logistic Regression</td>\n",
       "      <td>0.719210</td>\n",
       "      <td>0.698091</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.2253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>M30 Logistic Regression</td>\n",
       "      <td>0.721592</td>\n",
       "      <td>0.697905</td>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.2255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>M31 Logistic Regression</td>\n",
       "      <td>0.722127</td>\n",
       "      <td>0.697809</td>\n",
       "      <td>0.2194</td>\n",
       "      <td>0.2254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Training AUC  Validation AUC  Training RMSLE  \\\n",
       "0    M1 Logistic Regression      0.548108        0.555135          0.2271   \n",
       "1    M2 Logistic Regression      0.624687        0.627810          0.2253   \n",
       "2    M3 Logistic Regression      0.682657        0.686424          0.2225   \n",
       "3    M4 Logistic Regression      0.686342        0.688129          0.2224   \n",
       "4    M5 Logistic Regression      0.687915        0.684988          0.2223   \n",
       "5    M6 Logistic Regression      0.693311        0.694309          0.2220   \n",
       "6    M7 Logistic Regression      0.694318        0.695099          0.2219   \n",
       "7    M8 Logistic Regression      0.695176        0.696331          0.2219   \n",
       "8    M9 Logistic Regression      0.695775        0.694353          0.2218   \n",
       "9   M10 Logistic Regression      0.699305        0.693886          0.2216   \n",
       "10  M11 Logistic Regression      0.699465        0.694220          0.2216   \n",
       "11  M12 Logistic Regression      0.692398        0.697022          0.2220   \n",
       "12  M13 Logistic Regression      0.701580        0.700482          0.2213   \n",
       "13  M14 Logistic Regression      0.695972        0.694173          0.2218   \n",
       "14  M15 Logistic Regression      0.702613        0.699578          0.2212   \n",
       "15  M16 Logistic Regression      0.709128        0.701346          0.2206   \n",
       "16  M17 Logistic Regression      0.697060        0.688289          0.2216   \n",
       "17  M18 Logistic Regression      0.703661        0.687535          0.2211   \n",
       "18  M19 Logistic Regression      0.710787        0.688838          0.2205   \n",
       "19  M20 Logistic Regression      0.716079        0.695409          0.2199   \n",
       "20  M21 Logistic Regression      0.718754        0.696427          0.2197   \n",
       "21  M22 Logistic Regression      0.695714        0.696995          0.2219   \n",
       "22  M23 Logistic Regression      0.697076        0.695949          0.2218   \n",
       "23  M24 Logistic Regression      0.698787        0.695604          0.2216   \n",
       "24  M25 Logistic Regression      0.708691        0.705051          0.2208   \n",
       "25  M26 Logistic Regression      0.715142        0.706303          0.2201   \n",
       "26  M27 Logistic Regression      0.716938        0.698923          0.2199   \n",
       "27  M28 Logistic Regression      0.719412        0.698379          0.2197   \n",
       "28  M29 Logistic Regression      0.719210        0.698091          0.2197   \n",
       "29  M30 Logistic Regression      0.721592        0.697905          0.2195   \n",
       "30  M31 Logistic Regression      0.722127        0.697809          0.2194   \n",
       "\n",
       "    Validation RMSLE  \n",
       "0             0.2314  \n",
       "1             0.2291  \n",
       "2             0.2259  \n",
       "3             0.2259  \n",
       "4             0.2260  \n",
       "5             0.2255  \n",
       "6             0.2253  \n",
       "7             0.2251  \n",
       "8             0.2252  \n",
       "9             0.2251  \n",
       "10            0.2251  \n",
       "11            0.2251  \n",
       "12            0.2245  \n",
       "13            0.2252  \n",
       "14            0.2244  \n",
       "15            0.2247  \n",
       "16            0.2261  \n",
       "17            0.2259  \n",
       "18            0.2262  \n",
       "19            0.2258  \n",
       "20            0.2255  \n",
       "21            0.2251  \n",
       "22            0.2251  \n",
       "23            0.2252  \n",
       "24            0.2240  \n",
       "25            0.2244  \n",
       "26            0.2253  \n",
       "27            0.2252  \n",
       "28            0.2253  \n",
       "29            0.2255  \n",
       "30            0.2254  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model_name, features in models.items():\n",
    "    # Append \"Logistic Regression\" to the model name for clarity\n",
    "    full_model_name = f\"{model_name} Logistic Regression\"\n",
    "\n",
    "    # Define steps for pipeline: feature scaling and logistic regression\n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale\", StandardScaler(), features)], remainder='drop')),\n",
    "        (\"log_reg\", LogisticRegression())\n",
    "    ]\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Fit the model on training data\n",
    "    pipeline.fit(X_train[features], y_train)\n",
    "\n",
    "    # Predict probabilities on the training and validation data\n",
    "    # Note: We use predict_proba to get probabilities, and we're interested in the probabilities of the positive class (usually at index 1)\n",
    "    train_prob = pipeline.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = pipeline.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    results.append([full_model_name, train_auc, val_auc, train_rmsle, val_rmsle])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dcb08f-8c74-4005-896d-cc95dc41a487",
   "metadata": {},
   "source": [
    "### Tuned Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c183b2-d23b-4198-a4a3-4b3b9ab16f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed M1 in 0.98 seconds\n",
      "Completed M2 in 1.91 seconds\n",
      "Completed M3 in 4.39 seconds\n",
      "Completed M4 in 4.96 seconds\n",
      "Completed M5 in 6.05 seconds\n",
      "Completed M6 in 10.84 seconds\n",
      "Completed M7 in 12.25 seconds\n",
      "Completed M8 in 17.74 seconds\n"
     ]
    }
   ],
   "source": [
    "for model_name, features in models.items():\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Define steps for pipeline: feature scaling and logistic regression\n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale\", StandardScaler(), features)], remainder='drop')),\n",
    "        (\"log_reg\", LogisticRegression(solver='liblinear'))\n",
    "    ]\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Define a range of inverse regularization strength `C`\n",
    "    param_grid = {\n",
    "        'log_reg__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'log_reg__penalty': ['l2']  # L2 regularization\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(X_train[features], y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    train_prob = best_model.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = best_model.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    results.append([f\"{model_name} Logistic Regression Tuned\", train_auc, val_auc, train_rmsle, val_rmsle])\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {model_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4cc9c-847e-4cf5-9489-42a1a836189a",
   "metadata": {},
   "source": [
    "## Lasso Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8aff7a-c8f9-410f-9b69-231748b00629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale_numeric_features\", MinMaxScaler(), features)], remainder='drop')),\n",
    "        (\"lasso\", LassoCV())\n",
    "    ]\n",
    "    pipe_lasso = Pipeline(steps)\n",
    "    pipe_lasso.fit(X_train[features], y_train)\n",
    "\n",
    "    train_scores = pipe_lasso.predict(X_train[features])\n",
    "    val_scores = pipe_lasso.predict(X_val[features])\n",
    "\n",
    "    # Convert scores to binary predictions based on the median threshold\n",
    "    threshold = np.median(train_scores)\n",
    "    train_pred = np.where(train_scores > threshold, 1, 0)\n",
    "    val_pred = np.where(val_scores > threshold, 1, 0)\n",
    "\n",
    "    # Calculate AUC scores\n",
    "    train_auc = roc_auc_score(y_train, train_pred)\n",
    "    val_auc = roc_auc_score(y_val, val_pred)\n",
    "    \n",
    "    train_rmsle = calculateRMSLE(train_pred, y_train) \n",
    "    val_rmsle = calculateRMSLE(val_pred, y_val)\n",
    "\n",
    "    new_row = pd.DataFrame([[f\"{group_name} Lasso\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceeb0c7-2024-4301-87fb-ebfc56927fc8",
   "metadata": {},
   "source": [
    "## Stacking Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976a2a0-a0ea-424a-9fcc-d6310ec86a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define your base models\n",
    "base_models = [\n",
    "    ('dt', DecisionTreeClassifier(random_state=20240407)),\n",
    "    ('rf', RandomForestClassifier(random_state=20240407)),\n",
    "    ('xgb', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=20240407))\n",
    "]\n",
    "\n",
    "# Meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Stacking classifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "\n",
    "for model_name, features in models.items():\n",
    "    start_time = time.time()  # Start timer\n",
    "    \n",
    "    # Create a pipeline with scaling and stacking model\n",
    "    pipeline = Pipeline([\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale\", StandardScaler(), features)], remainder='drop')),\n",
    "        (\"stacking\", stacking_model)\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    pipeline.fit(X_train[features], y_train)\n",
    "    \n",
    "    # Predict probabilities on the training and validation data\n",
    "    train_prob = pipeline.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = pipeline.predict_proba(X_val[features])[:, 1]\n",
    "    \n",
    "    # Calculate AUC\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "    \n",
    "    # Append results\n",
    "    new_row = pd.DataFrame([[f\"{model_name} STACKED\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {model_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3f4f8-56ac-483b-8dbf-0bb64ae96f02",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a17cf-b4fa-4096-82dc-bb0a5919ec2d",
   "metadata": {},
   "source": [
    "### Decision Tree Classifer Max Depth 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49202df9-45fb-4893-b0ea-a50cd4f27365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Start timer\n",
    "    \n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale_numeric_features\", MinMaxScaler(), features)], remainder='drop')),\n",
    "        (\"tree\", DecisionTreeClassifier(max_depth=5, random_state=20240407))\n",
    "    ]\n",
    "    pipe_tree = Pipeline(steps)\n",
    "\n",
    "    # Fit the model on training data\n",
    "    pipe_tree.fit(X_train[features], y_train)\n",
    "\n",
    "    # Predict probabilities for the positive class\n",
    "    train_prob = pipe_tree.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = pipe_tree.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC scores\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    new_row = pd.DataFrame([[f\"{group_name} Decision Tree MD5\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba979487-f9cc-4aae-ad61-e68d1dc0e9d2",
   "metadata": {},
   "source": [
    "### Decision Tree Classifer Max Depth 6\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fe38d-609f-4d7b-8f50-50eaaf5c9b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Start timer\n",
    "    \n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale_numeric_features\", MinMaxScaler(), features)], remainder='drop')),\n",
    "        (\"tree\", DecisionTreeClassifier(max_depth=6, random_state=20240407))\n",
    "    ]\n",
    "    pipe_tree = Pipeline(steps)\n",
    "\n",
    "    # Fit the model on training data\n",
    "    pipe_tree.fit(X_train[features], y_train)\n",
    "\n",
    "    # Predict probabilities for the positive class\n",
    "    train_prob = pipe_tree.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = pipe_tree.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC scores using the probabilities\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    new_row = pd.DataFrame([[f\"{group_name} Decision Tree MD6\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306d27e3-0612-48ff-8a2a-ef4b0e80736f",
   "metadata": {},
   "source": [
    "### Decision Tree Classifer Max Depth 7\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6618b7-563a-4edb-95c4-6485c147e40d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Start timer\n",
    "    \n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale_numeric_features\", MinMaxScaler(), features)], remainder='drop')),\n",
    "        (\"tree\", DecisionTreeClassifier(max_depth=7, random_state=20240407))\n",
    "    ]\n",
    "    pipe_tree = Pipeline(steps)\n",
    "\n",
    "    # Fit the model on training data\n",
    "    pipe_tree.fit(X_train[features], y_train)\n",
    "\n",
    "    # Predict probabilities for the positive class\n",
    "    train_prob = pipe_tree.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = pipe_tree.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC scores using the probabilities\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    new_row = pd.DataFrame([[f\"{group_name} Decision Tree MD7\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9715ab5-3daa-46cc-9432-fcec6edd2462",
   "metadata": {},
   "source": [
    "### Decision Tree Classifer Grid Search\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e04bf-e77b-4458-aa04-3b3c24cadba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Start timer\n",
    "    \n",
    "    # Define the steps of the pipeline\n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale_numeric_features\", MinMaxScaler(), features)], remainder='drop')),\n",
    "        (\"tree\", DecisionTreeClassifier(random_state=20240407))\n",
    "    ]\n",
    "    pipe_tree = Pipeline(steps)\n",
    "    \n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        \"tree__max_depth\": range(3, 9) \n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipe_tree, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    # Fit the model on training data\n",
    "    grid_search.fit(X_train[features], y_train)\n",
    "    \n",
    "    # Best model after grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predict probabilities for the positive class with the best model\n",
    "    train_prob = best_model.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = best_model.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC scores using the probabilities\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    best_depth = best_model.named_steps['tree'].max_depth\n",
    "    new_row = pd.DataFrame([[f\"{group_name} Decision Tree Grid Search\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} with best max_depth={best_depth} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9357f0-e591-4fd6-884d-9158a6ab6321",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd91c4-1da4-4267-92a4-aad41ccf1ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Start timer\n",
    "    \n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale_numeric_features\", MinMaxScaler(), features)], remainder='drop')),\n",
    "        (\"random_forest\", RandomForestClassifier(random_state=20240407))\n",
    "    ]\n",
    "    pipe_rf = Pipeline(steps)\n",
    "    \n",
    "    # Define the parameter grid to search over\n",
    "    param_grid = {\n",
    "        \"random_forest__max_depth\": [None, 3, 5, 7],  # None means no limit on the depth\n",
    "        \"random_forest__n_estimators\": [10, 50, 100],  # Number of trees\n",
    "        \"random_forest__min_samples_split\": [2, 4]  # Minimum number of samples required to split an internal node\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipe_rf, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    # Fit the model on training data\n",
    "    grid_search.fit(X_train[features], y_train)\n",
    "    \n",
    "    # Best model after grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predict probabilities for the positive class with the best model\n",
    "    train_prob = best_model.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = best_model.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC scores using the probabilities\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    best_params = grid_search.best_params_\n",
    "    new_row = pd.DataFrame([[f\"{group_name} Random Forest\", train_auc, val_auc, train_rmsle, val_rmsle]],\n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} with best parameters {best_params} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b239e1-c123-43da-be03-90ed65695d1c",
   "metadata": {},
   "source": [
    "## Gradient Boosted Random Forest\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f3cc0-74f7-437a-8777-d21e91ec2370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Timer start\n",
    "    \n",
    "    steps = [\n",
    "        (\"scale_features\", ColumnTransformer([(\"scale_numeric_features\", MinMaxScaler(), features)], remainder='drop')),\n",
    "        (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "    ]\n",
    "    pipe_xgb = Pipeline(steps)\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        \"xgb__n_estimators\": [100, 200],  # Number of trees\n",
    "        \"xgb__max_depth\": [3, 5, 7],  # Depth of trees\n",
    "        \"xgb__learning_rate\": [0.01, 0.1]  # Step size shrinkage used in update to prevents overfitting\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(pipe_xgb, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    grid_search.fit(X_train[features], y_train)\n",
    "    \n",
    "    # Best model after grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predict probabilities for the positive class\n",
    "    train_prob = best_model.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = best_model.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC scores using the probabilities\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    best_params = grid_search.best_params_\n",
    "    new_row = pd.DataFrame([[f\"{group_name} XGBoost\", train_auc, val_auc, train_rmsle, val_rmsle]],\n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} with best parameters {best_params} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc04ef-9082-452d-9f45-307fc6502be1",
   "metadata": {},
   "source": [
    "## Light Gradient Boosting Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd1648-a056-4bc8-b100-97748763ecb9",
   "metadata": {},
   "source": [
    "### Simple Light Gradient Boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7502d19-7203-434a-8e59-be7e9bdb21f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create datasets for LightGBM\n",
    "    lgb_train = lgb.Dataset(X_train[features], label=y_train)\n",
    "    lgb_val = lgb.Dataset(X_val[features], label=y_val, reference=lgb_train)\n",
    "\n",
    "    # Simplify params by only setting the essentials\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbose': -1,\n",
    "        'random_state': 20240325\n",
    "    }\n",
    "\n",
    "    # Train model with a fixed number of boost rounds to simplify\n",
    "    num_boost_round = 100\n",
    "    lgb_model = lgb.train(params,\n",
    "                          lgb_train,\n",
    "                          num_boost_round=num_boost_round,\n",
    "                          valid_sets=[lgb_val])\n",
    "\n",
    "    # Prediction and evaluation\n",
    "    train_prob = lgb_model.predict(X_train[features], num_iteration=lgb_model.best_iteration)\n",
    "    val_prob = lgb_model.predict(X_val[features], num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "    # Assuming calculateRMSLE is previously defined\n",
    "    train_rmsle = calculateRMSLE(y_train, train_prob)\n",
    "    val_rmsle = calculateRMSLE(y_val, val_prob)\n",
    "\n",
    "    # Append results\n",
    "    new_row = pd.DataFrame([[f\"{group_name} LightGBM Simple\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df191341-84c1-438a-bf30-29d0efaac611",
   "metadata": {},
   "source": [
    "### Tuned Light Gradient Boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484226b4-43c1-4245-b4fd-dcf27f3e4e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create datasets for LightGBM\n",
    "    lgb_train = lgb.Dataset(X_train[features], label=y_train)\n",
    "    lgb_val = lgb.Dataset(X_val[features], label=y_val, reference=lgb_train)\n",
    "\n",
    "    # Adjust parameters to reduce overfitting\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.05,  # Lowered learning rate\n",
    "        'num_leaves': 20,  # Fewer leaves\n",
    "        'lambda_l1': 0.5,  # Added L1 regularization\n",
    "        'lambda_l2': 0.5,  # Added L2 regularization\n",
    "        'verbose': -1,\n",
    "        'random_state': 20240325\n",
    "    }\n",
    "\n",
    "    # Train model with early stopping\n",
    "    lgb_model = lgb.train(params,\n",
    "                          lgb_train,\n",
    "                          valid_sets=[lgb_val],\n",
    "                          num_boost_round=1000)  # Maximum number of boosting rounds\n",
    "\n",
    "    # Prediction and evaluation\n",
    "    train_prob = lgb_model.predict(X_train[features], num_iteration=lgb_model.best_iteration)\n",
    "    val_prob = lgb_model.predict(X_val[features], num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "    # Assuming calculateRMSLE is previously defined\n",
    "    train_rmsle = calculateRMSLE(y_train, train_prob)\n",
    "    val_rmsle = calculateRMSLE(y_val, val_prob)\n",
    "\n",
    "    # Append results\n",
    "    new_row = pd.DataFrame([[f\"{group_name} LightGBM Tuned\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "results_df.tail(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e039161-3d58-4514-a721-5e6eeca52055",
   "metadata": {},
   "source": [
    "## Cat Boosting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4088505-dece-4d4c-94a7-e2656528b98e",
   "metadata": {},
   "source": [
    "### Simple Cat Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad65ec-afd0-4b54-850c-ed85f8c233eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Defining CatBoost model\n",
    "    cb_model = CatBoostClassifier(\n",
    "        iterations=500,  # Fewer iterations for quicker learning\n",
    "        learning_rate=0.01,  # Higher learning rate for faster convergence\n",
    "        depth=4,  # Lower depth to reduce model complexity and overfitting\n",
    "        random_state=20240325,\n",
    "        verbose=False  # Silence the output to avoid flooding the notebook/console\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    cb_model.fit(X_train[features], y_train, eval_set=(X_val[features], y_val), early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    train_prob = cb_model.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = cb_model.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Assuming calculateRMSLE is defined elsewhere\n",
    "    train_rmsle = calculateRMSLE(y_train, train_prob)\n",
    "    val_rmsle = calculateRMSLE(y_val, val_prob)\n",
    "\n",
    "    # Append results\n",
    "    new_row = pd.DataFrame([[f\"{group_name} CatBoost Simple\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a4352-4d7d-43ee-bd70-5a142a985424",
   "metadata": {},
   "source": [
    "### Tuned Cat Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194de50-7e6a-466c-b409-a809dadc4ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    cb_model = CatBoostClassifier(\n",
    "        iterations=2000,  # Explore more iterations for deeper learning\n",
    "        learning_rate=0.001,  # Further reduce learning rate for more gradual learning\n",
    "        depth=7,  # Slightly increase depth for capturing more complex patterns\n",
    "        l2_leaf_reg=5,  # Increase L2 regularization to control overfit depth's complexity\n",
    "        bagging_temperature=1,  # Introduce bagging for randomness, reducing overfitting\n",
    "        early_stopping_rounds=100,\n",
    "        random_state=20240325,\n",
    "        verbose=False)  # Use only a portion of data for each tree, increasing diversity\n",
    "    \n",
    "    cb_model.fit(X_train[features], y_train, eval_set=(X_val[features], y_val), early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "    train_prob = cb_model.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = cb_model.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "\n",
    "    new_row = pd.DataFrame([[f\"{group_name} CatBoost Tuned\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a884036-8492-4ab0-b79d-01149db92e3e",
   "metadata": {},
   "source": [
    "## Explainable Boosting Machine\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89796388-b1e5-4406-b108-da038ed847c0",
   "metadata": {},
   "source": [
    "### Simple EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40007e87-defa-4e48-9e55-a512a9d43122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Timer start\n",
    "\n",
    "    # Adjusted EBM pipeline without SimpleImputer for numerical data\n",
    "    ebm = ExplainableBoostingClassifier(random_state=20240325)\n",
    "\n",
    "    ebm.fit(X_train[features], y_train)\n",
    "\n",
    "    # Predict probabilities for the positive class\n",
    "    train_prob = ebm.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = ebm.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC scores using the probabilities\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    new_row = pd.DataFrame([[f\"{group_name} EBM\", train_auc, val_auc, train_rmsle, val_rmsle]],\n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6b300-aa17-430b-862e-eb8403fdc830",
   "metadata": {},
   "source": [
    "### **Permutation Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b995f98-0f95-4dee-9edd-cd3ad6dbee52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose a model (for example, M1 EBM) and its features for illustration\n",
    "ebm = ExplainableBoostingClassifier(random_state=20240325)\n",
    "ebm.fit(X_train[models['M7']], y_train)\n",
    "\n",
    "# Compute permutation-based feature importance\n",
    "perm_importance = permutation_importance(ebm, X_val[models['M7']], y_val, n_repeats=10, random_state=42, scoring='roc_auc')\n",
    "\n",
    "# Retrieve and display feature importances\n",
    "feature_names = np.array(models['M7'])\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_names[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5811f-cbf2-4d2c-afae-d082685b9590",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming perm_importance is calculated as shown previously\n",
    "feature_names = np.array(models['M7'])  # Adjust to use the correct model features as needed\n",
    "\n",
    "# Identify features with positive permutation importance values\n",
    "positive_importance_features = feature_names[perm_importance.importances_mean > 0]\n",
    "\n",
    "# Print out the feature names\n",
    "print(\"Features with positive permutation importance:\")\n",
    "for feature in positive_importance_features:\n",
    "    print(feature)\n",
    "\n",
    "# Create a variable group with these features\n",
    "perm_importance_positive = positive_importance_features.tolist()\n",
    "\n",
    "print(\"Variable group with positive permutation importance:\")\n",
    "print(perm_importance_positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45731658-2c1f-4bb3-9ea7-e949bff18bf9",
   "metadata": {},
   "source": [
    "### Adjusted EBM 1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9ebe5-92b4-4d99-871a-7731a4d03813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()  # Timer start\n",
    "\n",
    "    # Adjusted EBM pipeline without SimpleImputer for numerical data\n",
    "    ebm_adjusted = ExplainableBoostingClassifier(\n",
    "        random_state=20240325,\n",
    "        learning_rate=0.01,\n",
    "        max_bins=256,\n",
    "        interactions=10,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    ebm_adjusted.fit(X_train[features], y_train)\n",
    "\n",
    "    # Predict probabilities for the positive class\n",
    "    train_prob = ebm_adjusted.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = ebm_adjusted.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    # Calculate AUC scores using the probabilities\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    # Append results\n",
    "    new_row = pd.DataFrame([[f\"{group_name} EBM Adjusted 1\", train_auc, val_auc, train_rmsle, val_rmsle]],\n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbaad88-bf56-4be3-be06-d69f2f84b484",
   "metadata": {},
   "source": [
    "### Adjusted EBM 2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4816a1-c3d0-4a73-b0eb-3daa4dc0453c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for group_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    ebm_more_adjusted = ExplainableBoostingClassifier(\n",
    "        random_state=20240325,\n",
    "        learning_rate=0.005,  # Slightly lower learning rate for more fine-grained adjustments\n",
    "        max_bins=512,  # Increased number of bins for potentially capturing more detail\n",
    "        interactions=15,  # Allowing for more interactions\n",
    "        early_stopping_rounds=100,  # More patience on early stopping to allow more rounds for convergence\n",
    "        n_jobs=-1  # Utilize all CPU cores for faster training\n",
    "    )\n",
    "\n",
    "    ebm_more_adjusted.fit(X_train[features], y_train)\n",
    "\n",
    "    train_prob = ebm_more_adjusted.predict_proba(X_train[features])[:, 1]\n",
    "    val_prob = ebm_more_adjusted.predict_proba(X_val[features])[:, 1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, train_prob)\n",
    "    val_auc = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "\n",
    "    new_row = pd.DataFrame([[f\"{group_name} EBM Adjusted 2\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {group_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0b07d-bede-4715-9b44-63dad2582cd8",
   "metadata": {},
   "source": [
    "## Neural Network Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf6c0b-0f6b-4aff-a8e9-2dd0f21c0a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "    \n",
    "# Reshape your data accordingly\n",
    "X_train_reshaped = X_train_scaled.reshape((-1, n_features, 1)) \n",
    "X_val_reshaped = X_val_scaled.reshape((-1, n_features, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bd9a9-5916-4582-942d-8fc149208216",
   "metadata": {},
   "source": [
    "### Simple Neural Network Model 1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141f855-6669-4799-aef5-5dc95be7b67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, features in models.items():\n",
    "    start_time = time.time()  # Timer start\n",
    "    \n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[AUC(name='auc')])\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0,\n",
    "              validation_data=(X_val_scaled, y_val),\n",
    "              callbacks=[EarlyStopping(monitor='val_auc', patience=3, restore_best_weights=True, mode='max')])\n",
    "\n",
    "    _, train_auc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "    _, val_auc = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "    \n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "    \n",
    "    new_row = pd.DataFrame([[f\"{group_name} NN Simple\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()  # End timer\n",
    "    print(f\"Completed {model_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630f1da-6084-492b-9beb-22f978372f3b",
   "metadata": {},
   "source": [
    "### Simple Neural Network Model 2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0de60-d318-43d9-8592-0df2d73b17fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dropout(0.5),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=[AUC(name='auc')])\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0,\n",
    "              validation_data=(X_val_scaled, y_val),\n",
    "              callbacks=[EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True, mode='max')])\n",
    "\n",
    "    _, train_auc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "    _, val_auc = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "    \n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "    \n",
    "    new_row = pd.DataFrame([[f\"{group_name} NN Simple 2\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {model_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0cd2bd-fd5a-4179-9e68-00b7590c59ae",
   "metadata": {},
   "source": [
    "### Simple Neural Network Model 3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44bcd0-0b7d-4863-9ca6-dfee985cab9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=[AUC(name='auc')])\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True, mode='max')\n",
    "    model.fit(X_train_scaled, y_train, epochs=150, batch_size=64, verbose=0,\n",
    "              validation_data=(X_val_scaled, y_val),\n",
    "              callbacks=[es])\n",
    "\n",
    "    train_pred = model.predict(X_train_scaled).flatten()\n",
    "    val_pred = model.predict(X_val_scaled).flatten()\n",
    "\n",
    "    _, train_auc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "    _, val_auc = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "    \n",
    "    train_rmsle = calculateRMSLE(y_train, np.clip(train_pred, 0, None))  # Clipping predictions to ensure non-negative values\n",
    "    val_rmsle = calculateRMSLE(y_val, np.clip(val_pred, 0, None))\n",
    "    \n",
    "    new_row = pd.DataFrame([[f\"{model_name} NN Simple 3\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {model_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e322c41-583f-48bd-9ed1-f0afb9ea9f22",
   "metadata": {},
   "source": [
    "### Complex Neural Network Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35a6b0-3a21-4eda-ac6f-2a8e4968ec37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=[AUC(name='auc')])\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=0,\n",
    "              validation_data=(X_val_scaled, y_val),\n",
    "              callbacks=[EarlyStopping(monitor='val_auc', patience=5, restore_best_weights=True, mode='max')])\n",
    "\n",
    "    _, train_auc = model.evaluate(X_train_scaled, y_train, verbose=0)\n",
    "    _, val_auc = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "    \n",
    "    train_rmsle = calculateRMSLE(train_prob, y_train)\n",
    "    val_rmsle = calculateRMSLE(val_prob, y_val)\n",
    "    \n",
    "    new_row = pd.DataFrame([[f\"{group_name} NN Complex\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {model_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb730fbc-6096-4fa2-a3b4-38ec9f54541f",
   "metadata": {},
   "source": [
    "### Conv1D Adjusted Neural Network 1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14a377-8620-4ffb-82c0-ba7cc9b6407e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Applying Conv1D on the reshaped data; treating each feature as a timestep\n",
    "        Conv1D(filters=32, kernel_size=1, activation='relu', input_shape=(n_features, 1)),\n",
    "        MaxPooling1D(pool_size=2, strides=2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss='binary_crossentropy', metrics=[AUC(name='auc')])\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_auc', patience=15, restore_best_weights=True, mode='max')\n",
    "    model.fit(X_train_reshaped, y_train, epochs=200, batch_size=32, verbose=0,\n",
    "              validation_data=(X_val_reshaped, y_val),\n",
    "              callbacks=[es])\n",
    "\n",
    "    _, train_auc = model.evaluate(X_train_reshaped, y_train, verbose=0)\n",
    "    _, val_auc = model.evaluate(X_val_reshaped, y_val, verbose=0)\n",
    "\n",
    "    # Prediction and RMSLE calculation need correct predictions\n",
    "    train_pred = model.predict(X_train_reshaped).flatten()\n",
    "    val_pred = model.predict(X_val_reshaped).flatten()\n",
    "\n",
    "    train_rmsle = calculateRMSLE(y_train, np.clip(train_pred, 0, None))\n",
    "    val_rmsle = calculateRMSLE(y_val, np.clip(val_pred, 0, None))\n",
    "\n",
    "    new_row = pd.DataFrame([[f\"{model_name} NN Conv1D Adjusted\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {model_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110ee8f3-0d35-4646-b9e3-76a78dd52a5a",
   "metadata": {},
   "source": [
    "### Conv1D Adjusted Neural Network 2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968b958-d4b3-4655-977b-9ac6829b9633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_name, features in models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(n_features, 1)), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=64, kernel_size=1, activation='relu'),  # Additional Conv layer\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.4),  # Slightly increased dropout\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005),  # Increased learning rate\n",
    "                  loss='binary_crossentropy', metrics=[AUC(name='auc')])\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True, mode='max')  # Adjusted patience\n",
    "    model.fit(X_train_reshaped, y_train, epochs=100, batch_size=64, verbose=0,  # Reduced epochs, increased batch size\n",
    "              validation_data=(X_val_reshaped, y_val),\n",
    "              callbacks=[es])\n",
    "\n",
    "    _, train_auc = model.evaluate(X_train_reshaped, y_train, verbose=0)\n",
    "    _, val_auc = model.evaluate(X_val_reshaped, y_val, verbose=0)\n",
    "\n",
    "    train_pred = model.predict(X_train_reshaped).flatten()\n",
    "    val_pred = model.predict(X_val_reshaped).flatten()\n",
    "\n",
    "    train_rmsle = calculateRMSLE(y_train, np.clip(train_pred, 0, None))\n",
    "    val_rmsle = calculateRMSLE(y_val, np.clip(val_pred, 0, None))\n",
    "\n",
    "    new_row = pd.DataFrame([[f\"{model_name} NN Conv1D Optimized 2\", train_auc, val_auc, train_rmsle, val_rmsle]], \n",
    "                           columns=['Model', 'Training AUC', 'Validation AUC', 'Training RMSLE', 'Validation RMSLE'])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Completed {model_name} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "results_df.tail(31)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a33e3-90b3-4668-a5a5-4165f874974d",
   "metadata": {},
   "source": [
    "# Hypertuning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bca990-82f6-4e01-b72d-7bf383d0bd02",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4abceeb-03e3-47d2-a333-c3d5d3d2582e",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b297dae-ec84-4c62-8e89-c73a29b7f3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add a 'Difference AUC' column to measure overfitting\n",
    "results_df['Difference AUC'] = abs(results_df['Training AUC'] - results_df['Validation AUC'])\n",
    "\n",
    "# Add a 'Complexity' column based on the model name. Assuming 'M1' is simpler than 'M11'.\n",
    "results_df['Complexity'] = results_df['Model'].apply(lambda x: int(x.split()[0][1:]))\n",
    "\n",
    "# Sort by Validation AUC (desc), then by Difference AUC (asc), then by Complexity (asc)\n",
    "sorted_results_df = results_df.sort_values(by=['Validation AUC', 'Difference AUC', 'Complexity'], ascending=[False, True, True])\n",
    "\n",
    "# Get the top 20 models\n",
    "top_20_models = sorted_results_df.head(20)\n",
    "top_20_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a0a1e-81b0-490d-9820-a230d43eb17e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sorting models by Validation RMSLE (ascending), then by Validation AUC (descending) for a focus on prediction accuracy\n",
    "sorted_by_rmsle_df = results_df.sort_values(by=['Validation RMSLE', 'Validation AUC'], ascending=[True, False])\n",
    "\n",
    "# Get the top 10 models focused on RMSLE\n",
    "top_20_models_rmsle = sorted_by_rmsle_df.head(20)\n",
    "print(\"Top 20 Models Sorted by RMSLE:\")\n",
    "top_20_models_rmsle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7299d-146e-41f7-a353-0b248bd6b938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalize RMSLE (assuming lower is better and to align with AUC's higher is better)\n",
    "max_rmsle = results_df['Validation RMSLE'].max()\n",
    "results_df['Normalized RMSLE'] = 1 - (results_df['Validation RMSLE'] / max_rmsle)\n",
    "\n",
    "# Simple combined score (example: 70% weight on AUC, 30% weight on Normalized RMSLE)\n",
    "results_df['Combined Score'] = 0.7 * results_df['Validation AUC'] + 0.3 * results_df['Normalized RMSLE']\n",
    "\n",
    "# Sort by combined score (descending)\n",
    "sorted_by_combined_score_df = results_df.sort_values(by='Combined Score', ascending=False)\n",
    "\n",
    "# Get the top 20 models based on the combined score\n",
    "top_20_models_combined = sorted_by_combined_score_df.head(20)\n",
    "print(\"Top 20 Models Sorted by Combined Score (AUC & RMSLE):\")\n",
    "top_20_models_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916fee6-0b25-4e08-81c1-99e1c32e8bc0",
   "metadata": {},
   "source": [
    "# Test Set Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d3cc89-8289-41ef-95a4-7e98813f87d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction_folder(day):\n",
    "    folder_path = f'Predictions/Day_{day}'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be16c003-cfae-4a96-89f2-5d0f8d85811a",
   "metadata": {},
   "source": [
    "### Prediction Functions\n",
    "---\n",
    "\n",
    "Since some models are repeated frequently, it will clean up the code to utilize functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51128a90-6832-4199-a7f5-9adabb966cae",
   "metadata": {},
   "source": [
    "#### Simple EBM Prediction Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e15e990-77bb-4889-b96d-e8cb03c6d0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_ebm_prediction(model, day):\n",
    "    features = models[model]\n",
    "\n",
    "    # Training the \"M9\" EBM model\n",
    "    ebm = ExplainableBoostingClassifier(random_state=20240325)\n",
    "    ebm.fit(X_train[features], y_train)\n",
    "\n",
    "    X_test = test_data[features]\n",
    "\n",
    "    # Predicting with the model\n",
    "    test_data['score'] = ebm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Saving the required predictions\n",
    "    test_data[['article_id', 'score']].to_csv(f'Predictions/Day_{day}/{model}_ebm_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc80151-89ec-4e2c-864f-af3ef9253af5",
   "metadata": {},
   "source": [
    "#### Adjusted EBM 1 Prediction Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241c519-6ae0-43cd-9b86-f63fa072e51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ebm_adjusted_1_prediction(model, day):\n",
    "    features = models[model]\n",
    "\n",
    "    # Adjusted EBM Model 1\n",
    "    ebm_adjusted_1 = ExplainableBoostingClassifier(\n",
    "        random_state=20240325,\n",
    "        learning_rate=0.01,\n",
    "        max_bins=256,\n",
    "        interactions=10,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    ebm_adjusted_1.fit(X_train[features], y_train)\n",
    "\n",
    "    X_test = test_data[features]\n",
    "\n",
    "    # Predicting with the model\n",
    "    test_data['score'] = ebm_adjusted_1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Saving the required predictions\n",
    "    test_data[['article_id', 'score']].to_csv(f'Predictions/Day_{day}/{model}_ebm_adjusted_1_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d9423-1ad8-4191-be0f-86b9a1fb6382",
   "metadata": {},
   "source": [
    "#### Adjusted EBM 2 Prediction Function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92c988-e1b7-4600-9be7-265118217eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ebm_adjusted_2_prediction(model, day):\n",
    "    features = models[model]\n",
    "\n",
    "    # Adjusted EBM Model 2\n",
    "    ebm_adjusted_2 = ExplainableBoostingClassifier(\n",
    "        random_state=20240325,\n",
    "        learning_rate=0.005,\n",
    "        max_bins=512,\n",
    "        interactions=15,\n",
    "        early_stopping_rounds=100,\n",
    "        n_jobs=-1  # Utilize all available CPU cores\n",
    "    )\n",
    "    ebm_adjusted_2.fit(X_train[features], y_train)\n",
    "\n",
    "    X_test = test_data[features]\n",
    "\n",
    "    # Predicting with the model\n",
    "    test_data['score'] = ebm_adjusted_2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Saving the required predictions\n",
    "    test_data[['article_id', 'score']].to_csv(f'Predictions/Day_{day}/{model}_ebm_adjusted_2_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a9969-0199-4c71-a1a6-0de09ca96977",
   "metadata": {},
   "source": [
    "## Day 1 Predictions\n",
    "---\n",
    "\n",
    "All of the predictions from day one came from the simple EBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7ec3f-ef88-4105-9a89-359b63a865ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_folder('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7eca6-7757-473c-ac21-85ce3412745c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simple EBM M9 Prediction \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5f35c-025a-4a49-b476-a1e10798c7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_ebm_prediction('M9', '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cd996-075c-4207-b15b-55fd5b3dcfed",
   "metadata": {},
   "source": [
    "### Simple EBM M7 Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4f6b1-cc52-44d5-9ff9-e7378f1dcaa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_ebm_prediction('M7', '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8a201-2dc8-4472-8910-5c26672c320d",
   "metadata": {},
   "source": [
    "### Simple EBM M10 Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640bec7-6839-4304-a312-ff78f4a35e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_ebm_prediction('M10', '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22a36c-e64e-4b48-af16-a9db67e1738c",
   "metadata": {},
   "source": [
    "### Simple EBM M6 Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a2a93-12d6-4bde-b63c-83b1314e055f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_ebm_prediction('M6', '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1d2a4-4966-4126-99e9-2dd500e326d7",
   "metadata": {},
   "source": [
    "### Simple EBM M12 Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1782da-5922-40db-8a98-d9c7fe177828",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ebm_prediction('M12', '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a8ebf-39a7-4557-a348-b2980cc1296a",
   "metadata": {},
   "source": [
    "## Day 2 Predictions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd9485-3d8b-405c-90da-bfcbe120ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_folder('2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122fc51-fc59-4e87-8a89-0c0d5b0ab95d",
   "metadata": {},
   "source": [
    "### Simple EBM M11 Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75a49f-bc44-4f08-b3a9-850e5077462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ebm_prediction('M11', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548c9a2-d6e2-455c-8277-fdf54a080634",
   "metadata": {},
   "source": [
    "### Adjusted EBM 1 M10 Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9346b-db13-4992-8b92-cecdd0a78779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ebm_adjusted_1_prediction('M10', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c4320b-a0b0-487e-b7cb-698dd549fdad",
   "metadata": {},
   "source": [
    "### Adjusted EBM 1 M11 Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606dd9a7-ff14-4d28-95fd-46e588bcecd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ebm_adjusted_1_prediction('M11', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e694ec-5202-4506-9317-fc84c2711f3a",
   "metadata": {},
   "source": [
    "### Adjusted EBM 1 M12 Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64caee-9a1d-497e-8045-d5be6d1325f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ebm_adjusted_1_prediction('M12', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e1321e-8e8d-4971-b5a3-853aee44ba9c",
   "metadata": {},
   "source": [
    "### Adjusted EBM 1 M9 Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cdafe-116a-4270-a4d7-84f86a5d43f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ebm_adjusted_1_prediction('M9', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860f173-5a64-43a1-bdda-a2e8bdfdb132",
   "metadata": {},
   "source": [
    "## Day 3 Predictions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f60648-ae4a-4c25-9d73-dcd9a0b8438d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_folder('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dbba8a-4cbe-468c-b5ad-82e5b9314a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_adjusted_1_prediction('M19', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a40e0-547a-4553-ae3d-8d5d82cff8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ebm_adjusted_2_prediction('M18', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0798461-4df9-4e2c-b3dd-6d14ad874eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ebm_adjusted_2_prediction('M19', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832b909-3f29-4ce7-b245-f4667dafc3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ebm_adjusted_2_prediction('M20', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5183bf7-11df-41ee-811d-ed0758dfc902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_ebm_prediction('M18', '3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae7c74-011d-4cc6-8ded-204749a47062",
   "metadata": {},
   "source": [
    "## Day 4 Predictions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024cbe27-febd-4b12-bc2c-6542d766fc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_folder('4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62392dd9-ee45-45ef-89fa-138619ef78d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
